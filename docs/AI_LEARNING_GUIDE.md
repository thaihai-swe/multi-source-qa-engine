# ğŸ“ Complete AI/ML Learning Guide for RAG System
## From Software Engineer to AI Engineer
### **46 Techniques & Concepts Explained**

**Version 3.0** | Updated: February 2026
**Audience:** New AI engineers transitioning from software engineering
**Scope:** Complete coverage of all 46 techniques and concepts used in this production RAG system
**Duration:** Comprehensive learning material (~8-10 hours to master thoroughly)

---

## ğŸ“š What You'll Master

This guide explains **every single technique and concept** used in our RAG system:
- âœ… **11 Core ML/AI concepts** - RAG, embeddings, cosine similarity, semantic search, etc.
- âœ… **11 Search & retrieval techniques** - Hybrid search, BM25, two-stage retrieval, cross-encoders, MMR
- âœ… **3 Advanced reasoning patterns** - Query expansion, multi-hop, self-query decomposition
- âœ… **7 Evaluation & quality metrics** - RAGAS, hallucination detection, fact checking, grounding
- âœ… **5 Data processing methods** - ChromaDB, adaptive chunking, tokenization, LRU caching
- âœ… **6 Engineering patterns** - Orchestrator, strategy, factory, ABC, dataclasses
- âœ… **Plus**: Safety features, LLM integration, performance optimizations, testing strategies

**Each concept includes:**
- ğŸ¯ Clear definition for beginners
- ğŸ’¡ Why it matters in production
- ğŸ”§ How we implement it in our system
- ğŸ“‚ Where to find it in the code
- âœï¸ Practical examples and exercises

---

## Table of Contents

### PART 0: Quick Reference
- [Glossary](#glossary) â€” All 46 concepts at a glance
- [System Architecture Map](#system-architecture-map) â€” Visual overview

### PART 1: Core ML/AI Foundations (11 Concepts)
1.1 [RAG (Retrieval-Augmented Generation)](#11-rag-retrieval-augmented-generation)
1.2 [Large Language Models (LLMs)](#12-large-language-models-llms)
1.3 [Vector Embeddings](#13-vector-embeddings)
1.4 [Cosine Similarity](#14-cosine-similarity)
1.5 [Semantic Search](#15-semantic-search)
1.6 [Prompt Engineering](#16-prompt-engineering)
1.7 [Temperature Control](#17-temperature-control)
1.8 [Context Window](#18-context-window)
1.9 [Tokens & Tokenization](#19-tokens--tokenization)
1.10 [Grounding & Source Attribution](#110-grounding--source-attribution)
1.11 [Hallucination in LLMs](#111-hallucination-in-llms)

### PART 2: Search & Retrieval Techniques (11 Concepts)
2.1 [Hybrid Search](#21-hybrid-search)
2.2 [BM25 Keyword Search](#22-bm25-keyword-search)
2.3 [Two-Stage Retrieval](#23-two-stage-retrieval)
2.4 [Cross-Encoder Reranking](#24-cross-encoder-reranking)
2.5 [Bi-Encoder (Sentence-BERT)](#25-bi-encoder-sentence-bert)
2.6 [MMR (Maximal Marginal Relevance)](#26-mmr-maximal-marginal-relevance)
2.7 [Jaccard Similarity](#27-jaccard-similarity)
2.8 [Passage Highlighting](#28-passage-highlighting)
2.9 [Batch Processing](#29-batch-processing)
2.10 [LRU Caching](#210-lru-caching)
2.11 [Graceful Degradation](#211-graceful-degradation)

### PART 3: Reasoning Patterns (3 Concepts)
3.1 [Query Expansion](#31-query-expansion)
3.2 [Multi-Hop Reasoning](#32-multi-hop-reasoning)
3.3 [Self-Query Decomposition](#33-self-query-decomposition)

### PART 4: Evaluation & Quality Metrics (7 Concepts)
4.1 [RAGAS Framework](#41-ragas-framework)
4.2 [Context Relevance](#42-context-relevance)
4.3 [Answer Relevance](#43-answer-relevance)
4.4 [Faithfulness](#44-faithfulness)
4.5 [Hallucination Detection](#45-hallucination-detection)
4.6 [Fact Checking](#46-fact-checking)
4.7 [Confidence Scoring](#47-confidence-scoring)

### PART 5: Data Processing & Storage (5 Concepts)
5.1 [ChromaDB Vector Database](#51-chromadb-vector-database)
5.2 [Adaptive Chunking](#52-adaptive-chunking)
5.3 [NLTK Tokenization](#53-nltk-tokenization)
5.4 [JSON Persistence](#54-json-persistence)
5.5 [Multi-Source Loading](#55-multi-source-loading)

### PART 6: Engineering Patterns (6 Concepts)
6.1 [Orchestrator Pattern](#61-orchestrator-pattern)
6.2 [Strategy Pattern](#62-strategy-pattern)
6.3 [Factory Pattern](#63-factory-pattern)
6.4 [Abstract Base Classes (ABC)](#64-abstract-base-classes-abc)
6.5 [Dataclasses](#65-dataclasses)
6.6 [Decorator Pattern](#66-decorator-pattern)

### PART 7: Safety & Production Features (3 Concepts)
7.1 [Domain Guard](#71-domain-guard)
7.2 [Streaming Responses](#72-streaming-responses)
7.3 [Adversarial Testing](#73-adversarial-testing)

### PART 8: Complete RAG Pipeline
8.1 [End-to-End Query Flow](#81-end-to-end-query-flow)
8.2 [Data Loading Pipeline](#82-data-loading-pipeline)
8.3 [Retrieval Pipeline](#83-retrieval-pipeline)
8.4 [Generation Pipeline](#84-generation-pipeline)
8.5 [Evaluation Pipeline](#85-evaluation-pipeline)

### PART 9: Practical Application
9.1 [Hands-On Exercises](#91-hands-on-exercises)
9.2 [Common Pitfalls & Solutions](#92-common-pitfalls--solutions)
9.3 [Performance Tuning Guide](#93-performance-tuning-guide)
9.4 [Production Deployment Checklist](#94-production-deployment-checklist)

### PART 10: Resources & Next Steps
10.1 [Research Papers](#101-research-papers)
10.2 [Tools & Frameworks](#102-tools--frameworks)
10.3 [Learning Path](#103-learning-path)
10.4 [Building on This System](#104-building-on-this-system)

---

# PART 0: QUICK REFERENCE

## GLOSSARY

**Quick reference for all 46 techniques and concepts.** Come back here when you encounter an unfamiliar term.

### Core ML/AI (11 concepts)

| #   | Concept                                  | Definition                                                                                                   | System Usage                        |
| --- | ---------------------------------------- | ------------------------------------------------------------------------------------------------------------ | ----------------------------------- |
| 1   | **RAG (Retrieval-Augmented Generation)** | Retrieve relevant documents from a knowledge base, then provide them to an LLM to generate grounded answers. | Our entire system architecture      |
| 2   | **LLM (Large Language Model)**           | Neural network trained on massive text data that generates human-like text (GPT-4, Llama, Claude).           | Answer generation via OpenAI API    |
| 3   | **Embedding**                            | Vector (list of numbers) representing text meaning. Similar texts â†’ similar vectors.                         | ChromaDB stores 768-dim vectors     |
| 4   | **Cosine Similarity**                    | Measures angle between two vectors. 1.0 = identical, 0.0 = unrelated.                                        | Semantic search scoring             |
| 5   | **Semantic Search**                      | Search by meaning, not keywords. "car" matches "automobile".                                                 | 70% of hybrid search                |
| 6   | **Prompt Engineering**                   | Crafting instructions/context for LLMs to get desired output.                                                | System prompts with grounding rules |
| 7   | **Temperature**                          | LLM parameter (0.0-2.0) controlling randomness. Lower = factual, higher = creative.                          | 0.2 for answers, 0.7 for expansion  |
| 8   | **Context Window**                       | Maximum text an LLM can process in one call (e.g., 4096 tokens).                                             | Limits retrieved chunks             |
| 9   | **Token**                                | Basic unit LLMs process (~0.75 words in English).                                                            | Chunking measured in tokens         |
| 10  | **Grounding**                            | Claims traced back to source documents. High grounding = low hallucination.                                  | Every answer cites sources          |
| 11  | **Hallucination**                        | LLM generating plausible but false information not in source material.                                       | Detected via grounding analysis     |

### Search & Retrieval (11 concepts)

| #   | Concept                              | Definition                                                                           | System Usage                  |
| --- | ------------------------------------ | ------------------------------------------------------------------------------------ | ----------------------------- |
| 12  | **Hybrid Search**                    | Combines semantic (70%) + keyword (30%) search for better coverage.                  | `HybridSearchEngine` class    |
| 13  | **BM25**                             | Statistical keyword ranking algorithm (Okapi BM25). TF-IDF-based with normalization. | 30% of hybrid search          |
| 14  | **Two-Stage Retrieval**              | Fast bi-encoder (50 docs) â†’ accurate cross-encoder (top 5). Speed + precision.       | Retrieve then rerank          |
| 15  | **Cross-Encoder**                    | Jointly encodes query + document for accurate scoring. Slow but precise.             | MS MARCO model reranking      |
| 16  | **Bi-Encoder**                       | Encodes query/docs separately, fast but less accurate.                               | ChromaDB semantic search      |
| 17  | **MMR (Maximal Marginal Relevance)** | Balances relevance + diversity: `Î»Ã—Relevance - (1-Î»)Ã—Similarity`.                    | After reranking (Î»=0.7)       |
| 18  | **Jaccard Similarity**               | `                                                                                    | Aâˆ©B                           | / | AâˆªB | ` - set-based similarity for diversity. | MMR diversity calculation |
| 19  | **Passage Highlighting**             | Extract most relevant sentences from documents for display.                          | `PassageHighlighter` class    |
| 20  | **Batch Processing**                 | Process multiple items simultaneously for efficiency.                                | Query variations in parallel  |
| 21  | **LRU Cache**                        | Least Recently Used cache. Evicts oldest items when full.                            | `EmbeddingCache` ~50% speedup |
| 22  | **Graceful Degradation**             | System works without optional dependencies, falls back to simpler methods.           | Cross-encoder optional        |

### Reasoning Patterns (3 concepts)

| #   | Concept                      | Definition                                                                       | System Usage                 |
| --- | ---------------------------- | -------------------------------------------------------------------------------- | ---------------------------- |
| 23  | **Query Expansion**          | Generate multiple query variations to broaden retrieval coverage.                | `QueryExpander` 4 variations |
| 24  | **Multi-Hop Reasoning**      | Break complex questions into 3 sequential sub-steps with intermediate retrieval. | `MultiHopReasoner` class     |
| 25  | **Self-Query Decomposition** | Split multi-aspect questions into parallel focused sub-queries.                  | `SelfQueryDecomposer` class  |

### Evaluation & Quality (7 concepts)

| #   | Concept                     | Definition                                                                            | System Usage                  |
| --- | --------------------------- | ------------------------------------------------------------------------------------- | ----------------------------- |
| 26  | **RAGAS**                   | Framework measuring RAG quality: context relevance + answer relevance + faithfulness. | `RAGASEvaluator` automatic    |
| 27  | **Context Relevance**       | Are retrieved documents relevant to query? `relevant_docs / total_docs`.              | RAGAS metric 1 of 3           |
| 28  | **Answer Relevance**        | Does answer address the question? Cosine similarity(query, answer).                   | RAGAS metric 2 of 3           |
| 29  | **Faithfulness**            | Are claims supported by context? `supported / total_claims`.                          | RAGAS metric 3 of 3           |
| 30  | **Hallucination Detection** | Claim-level grounding analysis with risk scoring (LOW/MEDIUM/HIGH).                   | `HallucinationDetector` class |
| 31  | **Fact Checking**           | Verify claims against retrieved context. SUPPORTED/CONTRADICTED/UNKNOWN.              | `FactChecker` class           |
| 32  | **Confidence Scoring**      | Multi-level confidence based on retrieval quality + metric agreement.                 | User trust indicator          |

### Data & Storage (5 concepts)

| #   | Concept                  | Definition                                                                        | System Usage              |
| --- | ------------------------ | --------------------------------------------------------------------------------- | ------------------------- |
| 33  | **ChromaDB**             | Vector database storing embeddings with fast approximate nearest neighbor search. | Persistent `./chroma_db/` |
| 34  | **Adaptive Chunking**    | Content-aware sizing: 800 (academic), 300 (structured), 500 (general) tokens.     | `AdaptiveChunker` class   |
| 35  | **NLTK Tokenization**    | Natural Language Toolkit. Word/sentence splitting.                                | Punkt tokenizer           |
| 36  | **JSON Persistence**     | Store conversation history + metrics as JSON for audit trail.                     | `JSONStorage` class       |
| 37  | **Multi-Source Loading** | Wikipedia API, web scraping (BeautifulSoup), PDFs (PyPDF2), local files.          | `MultiSourceDataLoader`   |

### Engineering Patterns (6 concepts)

| #   | Concept                   | Definition                                                            | System Usage                             |
| --- | ------------------------- | --------------------------------------------------------------------- | ---------------------------------------- |
| 38  | **Orchestrator Pattern**  | One class coordinates all components without tight coupling.          | `RAGSystem` central hub                  |
| 39  | **Strategy Pattern**      | Interchangeable algorithms selected at runtime.                       | `HybridSearchEngine` combines strategies |
| 40  | **Factory Pattern**       | Object creation logic based on input type.                            | `MultiSourceDataLoader` selects loader   |
| 41  | **Abstract Base Classes** | Interface contracts with `@abstractmethod`. Enforces implementation.  | `Storage`, `Retriever`, `Chunker`        |
| 42  | **Dataclasses**           | Type-safe data containers with auto-generated `__init__`, `__repr__`. | All models (`RAGResponse`, etc.)         |
| 43  | **Decorator Pattern**     | Wrap functionality without modifying core logic.                      | Feature toggles in CLI                   |

### Safety & Production (3 concepts)

| #   | Concept                 | Definition                                                                                 | System Usage                 |
| --- | ----------------------- | ------------------------------------------------------------------------------------------ | ---------------------------- |
| 44  | **Domain Guard**        | Detect out-of-domain queries against loaded source profile. Cosine similarity to centroid. | `DomainGuard` threshold 0.35 |
| 45  | **Streaming Responses** | Token-by-token output for real-time feedback instead of waiting.                           | OpenAI streaming API         |
| 46  | **Adversarial Testing** | Systematic edge-case testing (empty, impossible, contradictory queries).                   | `AdversarialSuite` 8 tests   |

---

## System Architecture Map

```
USER QUERY
    â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  PART 1: Core ML/AI (RAG, LLM, Embeddings, Semantic...)  â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                    â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  PART 2: Search & Retrieval                               â”‚
â”‚  Hybridâ†’ChromaDB(#33)+BM25(#13)â†’Rerank(#15)â†’MMR(#17)    â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                    â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  PART 3: Reasoning (Optional Enhancement)                 â”‚
â”‚  Query Expansion(#23) | Multi-Hop(#24) | Self-Query(#25) â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                    â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  GENERATION + EVALUATION                                   â”‚
â”‚  LLM(#2) â†’ RAGAS(#26) â†’ Hallucination(#30) â†’ Facts(#31) â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                    â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  PART 5: Storage & Output                                 â”‚
â”‚  JSON Persistence(#36) + Passage Highlighting(#19)       â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                    â†“
            ANSWER + SOURCES
```

**Engineering Foundation:**
- PART 6 patterns (Orchestrator #38, Strategy #39, Factory #40, ABC #41, Dataclasses #42, Decorator #43)
- PART 7 safety (Domain Guard #44, Streaming #45, Adversarial Testing #46)

---

# PART 1: CORE ML/AI FOUNDATIONS

> **What you'll learn:** The 11 fundamental AI/ML concepts that power the entire system. Start here if you're new to AI.
>
> **Time:** 90â€“120 minutes
>
> **Prerequisites:** Basic Python programming

---

## 1.1 RAG (Retrieval-Augmented Generation)

### What It Is

**RAG** is an architectural pattern that solves the fundamental problem: **LLMs don't know your custom data**.

```
WITHOUT RAG:
User: "What were our Q4 sales?"
LLM: "I don't have access to your company's sales data."

WITH RAG:
User: "What were our Q4 sales?"
System:
  1. Retrieves relevant documents from internal database
  2. Injects them as context for the LLM
  3. LLM generates answer based on provided context
LLM: "Based on the Q4 report, sales were $2.3M..."
```

### Why It Matters

| Problem              | Traditional LLM                         | RAG Solution                      |
| -------------------- | --------------------------------------- | --------------------------------- |
| **Knowledge cutoff** | Only knows data up to training date     | Retrieves current documents       |
| **Hallucination**    | Makes up plausible-sounding answers     | Grounds answers in real documents |
| **No citations**     | Can't cite sources                      | Returns source documents          |
| **Static knowledge** | Can't learn new info without retraining | Just add new documents            |
| **Privacy**          | Sends data to external APIs             | Keeps data local                  |

### How We Use It

**Our entire system is a production RAG implementation:**

```python
# src/core/rag_system.py - The heart of the system
class RAGSystem:
    def process_query(self, query: str) -> RAGResponse:
        # 1. RETRIEVE relevant documents
        documents = self.retriever.retrieve(query)

        # 2. AUGMENT LLM prompt with retrieved context
        prompt = self._build_prompt(query, documents)

        # 3. GENERATE answer from LLM
        answer = self.generator.generate(prompt)

        return RAGResponse(answer=answer, sources=documents)
```

**Three-stage pipeline:**
1. **Retrieve** (PART 2): Hybrid search finds relevant chunks from ChromaDB + BM25
2. **Augment** (PART 1.6): Build prompt with system instructions + retrieved context + query
3. **Generate** (PART 1.2): LLM produces grounded answer

### Where to Find It

| File                                     | Purpose                                            |
| ---------------------------------------- | -------------------------------------------------- |
| `src/core/rag_system.py`                 | Main RAG orchestrator - coordinates all components |
| `src/retrieval/hybrid_search.py`         | Retrieval stage - semantic + keyword search        |
| `src/generation/llm_answer_generator.py` | Generation stage - LLM API calls                   |
| `src/evaluation/ragas_evaluator.py`      | Evaluation - measures RAG quality                  |

### Example

```bash
# Terminal session showing RAG in action
> load wikipedia "Machine Learning"
   âœ… Loaded 42 chunks into ChromaDB

> query What is supervised learning?

   ğŸ” RETRIEVE: Found 3 relevant chunks about supervised learning
   ğŸ“ AUGMENT: Injected chunks into system prompt
   ğŸ¤– GENERATE: LLM answer based only on retrieved context

   Answer: Supervised learning is a type of machine learning where...
   Sources: [chunk_12, chunk_15, chunk_23]
```

### Exercise

**Try this:**
# You provide: (query, context, answer)
# System learns: "Is this a good answer to this query?"
evaluate_answer_relevance(query, answer)  # â† Learned from examples
```

### 2. Unsupervised Learning

**Definition:** Find patterns without labeled output.

```
Example: Cluster documents
  Input: 1000 documents
  Process: Find groups with similar topics
  Output: "5 document clusters found"

Your Project: Vector embeddings (unsupervised)
  - ChromaDB automatically groups similar concepts
  - No one told it what "goals" or "assists" means
  - It learned from patterns in training data
```

### 3. Reinforcement Learning

**Definition:** Learn by trial and error with rewards.

```
Example: Game AI
  Action: Move forward
  Environment: Reward (+10 points) or Penalty (-5 points)
  After 1M trials: Learns optimal strategy

Your Project: Adversarial testing is similar
  - Test case: Ambiguous query
  - Result: PASS/FAIL
  - System should avoid FAILURES next time
```

---

# PART 2: CORE TECHNOLOGIES

> **What you'll learn:** The four pillars of the tech stack â€” Python data structures for AI, how LLMs generate text, what vector embeddings are and why they enable semantic search, and how ChromaDB stores and retrieves vectors.
>
> **Prerequisites:** PART 1 or basic ML familiarity.
>
> **Time:** 60â€“90 min
>
> **Key insight:** After this part, the line `ChromaDB.query(embedding)` stops being magic and becomes a concrete operation you can reason about.

## 2.1 Python & Data Structures

### Why Python for AI?

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ Language         â”‚ AI Libraries    â”‚ Community    â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ Python           â”‚ TensorFlow,     â”‚ HUGE â–“â–“â–“â–“â–“â–“â–“â”‚
â”‚                  â”‚ PyTorch,        â”‚              â”‚
â”‚                  â”‚ NumPy, Pandas   â”‚ Job market   â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ Java             â”‚ Limited         â”‚ Small        â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ C++              â”‚ Good, but       â”‚ Research     â”‚
â”‚                  â”‚ harder to use   â”‚ only         â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### Key Data Structures in AI

#### Lists & Arrays
```python
# Traditional list (software engineer)
users = ["Alice", "Bob", "Charlie"]
users[0]  # "Alice"

# AI array - NumPy (for math operations)
import numpy as np
embeddings = np.array([0.2, 0.5, -0.1, 0.7])  # Vector embedding
embeddings * 2  # All elements multiplied by 2
np.dot(embeddings, other_embedding)  # Similarity score
```

#### Dictionaries (Key-Value Pairs)
```python
# Storing metadata about retrieved documents
document = {
    "content": "Ronaldo scored...",
    "source": "Wikipedia",
    "source_type": "wikipedia",
    "confidence": 0.87,
    "timestamp": "2026-02-25T10:30:00"
}

# Easy to access:
print(document["confidence"])  # 0.87
```

#### Dataclasses (Structured Data)
```python
# In your project (line 88):
@dataclass
class RetrievedDocument:
    content: str
    source: str
    source_type: str
    index: int
    distance: Optional[float] = None

# Why dataclasses?
# 1. Type hints (know what goes in each field)
# 2. Automatic __init__ (no boilerplate)
# 3. Easy serialization to JSON
# 4. Self-documenting code
```

**Comparison:**

```python
# Without dataclass (messy)
def create_doc(content, source, source_type, index, distance=None):
    return {
        "content": content,
        "source": source,
        ...
    }
# Hard to track what's required, easy to forget fields

# With dataclass (clean)
doc = RetrievedDocument(
    content="...",
    source="Wikipedia",
    source_type="wikipedia",
    index=0
)
# Type hints prevent errors, self-documenting
```

---

## 2.2 Large Language Models (LLMs)

### What is an LLM?

**Simple Definition:** A massive neural network trained to predict the next word.

```
Training Process:
  Input:  "The cat sat on the..."
  Predict: "mat"

  Input:  "Ronaldo scored..."
  Predict: "goals"

After predicting correctly on BILLIONS of examples, it
learns to generate coherent text.
```

### How LLMs Work (High Level)

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ 1. TOKENIZATION                         â”‚
â”‚ "Who is Ronaldo?" â†’ [Who] [is] [Ron]...â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ 2. EMBEDDING LAYER                      â”‚
â”‚ Each token â†’ Vector (meaning)           â”‚
â”‚ [Who]: [0.2, -0.5, 0.8, ...]           â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ 3. ATTENTION LAYERS (Main Processing)   â”‚
â”‚ Which words are important for this      â”‚
â”‚ context? Self-attention mechanism       â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ 4. TRANSFORMER LAYER (Repeated)         â”‚
â”‚ Process embeddings 12-96 times deeper   â”‚
â”‚ Extract more sophisticated patterns     â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ 5. OUTPUT LAYER                         â”‚
â”‚ Predict next token probability:         â”‚
â”‚ "mat": 0.6%, "has": 0.05%, etc         â”‚
â”‚ Pick highest probability â†’ Output       â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### In Your Project

```python
# src/models/config.py
OPEN_AI_API_BASE_URL = "http://127.0.0.1:1234/v1"
OPEN_AI_MODEL = "meta-llama-3.1-8b-instruct"

# You're running LLaMA locally (not using OpenAI)
# 8B = 8 Billion parameters (8 billion numbers to tune)
# Smaller but faster than GPT-4 (1.7T parameters)

# Used in src/generation/__init__.py:
response = client.chat.completions.create(
    model=OPEN_AI_MODEL,
    messages=[
        {"role": "system", "content": "You are helpful..."},
        {"role": "user", "content": "Who is Ronaldo?"}
    ],
    temperature=0.3,  # Lower = more predictable
    max_tokens=1000   # Limit output length
)
```

### Parameters Explained

**Temperature** (0.0 to 2.0):
```
Temperature = 0.0 (Always pick highest probability)
  "Ronaldo" â†’ "is" â†’ "a" â†’ "footballer"
  Deterministic, boring, factual âœ“

Temperature = 0.5 (Balanced)
  "Ronaldo" â†’ "is" â†’ "one of" â†’ "greatest"
  More natural, still coherent âœ“

Temperature = 1.0 (Random, but weighted)
  "Ronaldo" â†’ "plays" â†’ "the" â†’ "guitar"
  Creative, but may hallucinate âœ—

Temperature = 2.0 (Very random)
  Outputs become nonsense

Your Project: temperature=0.2 (very factual) â† Good for RAG
```

**max_tokens:**
```python
# 1 token â‰ˆ 4 characters (rough estimate)
max_tokens=1000  # Output max ~4000 characters

# Why limit it?
# 1. Cost (pay per token)
# 2. Prevent rambling answers
# 3. Faster response time
```

---

## 2.3 Vector Embeddings

### What are Embeddings?

**Core Idea:** Represent meaning as numbers.

```
Traditional Storage:
  "Ronaldo" â†’ String (just text, no meaning)
  "Messi"   â†’ String (just text, no meaning)
  Can't compute similarity (are they related?)

Embeddings:
  "Ronaldo" â†’ [0.2, 0.8, -0.3, 0.5, 0.9, ...]
  "Messi"   â†’ [0.25, 0.82, -0.28, 0.52, 0.91, ...]
  Can compute distance: How different are they?
```

### Mathematical Foundation

```
Embedding = Vector = Array of numbers

Dimensions typically: 384, 768, 1536 numbers

Example (simplified, only 4 dimensions):
  "player" concept â†’ [strong, human, career, sports]
  Ronaldo:  [0.9,   0.8,    0.95,    0.92]
  Messi:    [0.88,  0.82,   0.94,    0.90]
  Car:      [0.1,   0.05,   0.02,    0.15]

Similarity = How close are vectors?
  Ronaldo vs Messi: Very similar âœ“
  Ronaldo vs Car: Very different âœ—
```

### How to Calculate Similarity

#### 1. Cosine Similarity (Most Common)

```python
import numpy as np

def cosine_similarity(vec1, vec2):
    """Calculate angle between vectors (0=different, 1=identical)"""
    dot_product = np.dot(vec1, vec2)
    norm1 = np.linalg.norm(vec1)
    norm2 = np.linalg.norm(vec2)

    return dot_product / (norm1 * norm2)

ronaldo = np.array([0.9, 0.8, 0.95, 0.92])
messi = np.array([0.88, 0.82, 0.94, 0.90])
car = np.array([0.1, 0.05, 0.02, 0.15])

print(cosine_similarity(ronaldo, messi))   # 0.998 (very similar!)
print(cosine_similarity(ronaldo, car))     # 0.087 (very different)
```

**Why Cosine Similarity?**
```
Euclidean Distance (traditional):       Cosine Similarity (AI):
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”                     â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ (0,0) â†’ (3,4)   â”‚                     â”‚ Direction only  â”‚
â”‚ Distance: 5     â”‚                     â”‚ (0,0)â†’(3,4) sameâ”‚
â”‚                 â”‚                     â”‚ as (0,0)â†’(6,8)  â”‚
â”‚ Scale matters   â”‚                     â”‚ Scale doesn't   â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜                     â”‚ matter (both=1) â”‚
                                        â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

In RAG: We care about MEANING (direction), not magnitude
So cosine similarity is perfect âœ“
```

#### 2. Euclidean Distance

```python
def euclidean_distance(vec1, vec2):
    """Traditional distance between points"""
    return np.linalg.norm(vec1 - vec2)

# Used in some systems, less common for text
```

### In Your Project

```python
# src/retrieval/loader.py: Initialize embedding function
embedding_function = embedding_functions.DefaultEmbeddingFunction()

# Embedded automatically when adding to ChromaDB:
collection.add(
    documents=chunks,  # Text
    # ChromaDB converts to embeddings automatically
    # Each chunk â†’ Vector of 1536 numbers (OpenAI default)
)

# Search (line 985+):
response = collection.query(
    query_texts=[user_query],  # ChromaDB embeds this
    n_results=3
)
# ChromaDB compares user embedding to all document embeddings
# Returns top 3 most similar
```

---

## 2.4 ChromaDB (Vector Database)

### Why Vector Databases?

**Problem with Traditional Databases:**

```sql
-- PostgreSQL (traditional)
SELECT * FROM documents WHERE content LIKE '%Ronaldo%'
-- Only finds exact text matches
-- Misses: "He scored...", "The player...", "Famous footballer..."
```

**Solution with Vector Database:**

```python
# ChromaDB
collection.query(
    query_embeddings=[...],  # "Ronaldo achievements"
    n_results=3
)
# Finds similar concepts:
# - "Scored goals"
# - "Career records"
# - "International performance"
```

### How Vector Databases Work

```
1. STORAGE PHASE:
   Document: "Ronaldo scored 128 international goals"
   â†“ Convert to embedding
   Vector: [0.2, 0.5, -0.1, 0.7, ..., 0.3]
   â†“ Store in database with index structure
   ChromaDB stores this + metadata

2. RETRIEVAL PHASE:
   Query: "International goals"
   â†“ Convert to embedding
   Query Vector: [0.18, 0.52, -0.12, 0.71, ..., 0.28]
   â†“ Find nearest neighbors (using tree structure)
   âœ“ Finds: "Ronaldo 128 international goals" (similar vector)
```

### Index Structures

**Why Index?** Searching 1M vectors without index = 1M distance calculations = SLOW

```
Linear Search: Compare to all 1,000,000 vectors
â”œâ”€ Time: 1M comparisons âœ— TOO SLOW
â””â”€ Accuracy: 100% âœ“

HNSW (Hierarchical Navigable Small World):
â”œâ”€ Time: ~50-100 comparisons âœ“ FAST
â””â”€ Accuracy: 99.9% âœ“ GOOD ENOUGH
   (Sacrifices 0.1% accuracy for 10,000x speed)

Approximate Nearest Neighbor Search â† What ChromaDB uses
```

### In Your Project

```python
# src/retrieval/loader.py: Initialize ChromaDB
db_client = chromadb.PersistentClient(
    path="./chroma_db",  # Local persistent storage
    settings=Settings(
        anonymized_telemetry=False,
        allow_reset=True
    )
)

# src/core/__init__.py: Add documents
collection.add(
    ids=[f"chunk_{i}" for i in range(100)],
    documents=[chunk1, chunk2, ..., chunk100],  # Raw text
    metadatas=[
        {"source": "Wikipedia", "index": 0},
        ...
    ]
)
# ChromaDB automatically:
# 1. Converts each text â†’ embedding
# 2. Stores in vector database
# 3. Creates search index

# src/retrieval/hybrid_search.py: Query
response = collection.query(
    query_embeddings=embedding_function([user_query]),
    n_results=3
)
# Returns 3 most similar documents
```

### Key Concepts

**Collection:** Group of related documents
```python
# Like a database table, but for vectors
ronaldo_collection = db.get_or_create_collection("ronaldo_wikipedia")
messi_collection = db.get_or_create_collection("messi_wikipedia")
```

**Metadata:** Information about documents
```python
metadatas=[
    {
        "source": "Wikipedia",       # Where from?
        "source_type": "wikipedia",  # Type of source
        "index": 0,                  # Position in chunking
        "timestamp": "2026-02-25"    # When loaded?
    }
]
```

**Distance Metric:**
```python
# ChromaDB uses Euclidean distance by default
# Query result includes distance (0=identical, larger=different)
```

---

# PART 3: SEARCH & RETRIEVAL

> **What you'll learn:** How semantic search works using embeddings, how BM25 keyword search works, why combining them beats either alone (hybrid search), and two advanced retrieval strategies â€” query expansion and multi-hop reasoning.
>
> **Prerequisites:** PART 2 (especially Â§2.3 Vector Embeddings).
>
> **Time:** 90 min
>
> **After this part:** Run `expand "What is machine learning?"` and compare it to `query "What is machine learning?"` â€” you will see exactly why retrieval coverage differs.
>
> **Key source files:** `src/retrieval/hybrid_search.py`, `src/reasoning/query_expander.py`, `src/reasoning/multi_hop_reasoner.py`

## 3.1 Semantic Search

### Definition

**Semantic** = Relating to meaning (not just keywords).

```
Query: "How many goals?"

Keyword Search (FAILS):
  Looks for: "goals"
  Finds: "He scored" âœ— Missing word "goals"
  Misses: "His record of 128" âœ— No word "goals"

Semantic Search (SUCCEEDS):
  Understands: Query about achievement/numbers
  Finds: "scored 128" âœ“ Matches meaning
  Finds: "international goals record" âœ“ Matches meaning
  Finds: "hat-trick achievements" âœ“ Related meaning
```

### How Semantic Search Works

```
1. VECTORIZE QUERY
   "How many goals?" â†’ [0.2, 0.5, -0.1, 0.7, ...]

2. VECTORIZE ALL DOCUMENTS (done once, stored)
   "scored 128" â†’ [0.18, 0.52, -0.12, 0.71, ...]
   "records" â†’ [0.22, 0.48, -0.09, 0.75, ...]

3. CALCULATE SIMILARITY
   Distance("goals vector", "scored 128 vector") = 0.05 â† Close!
   Distance("goals vector", "birthplace vector") = 2.1 â† Far!

4. RETURN TOP N MOST SIMILAR
   Returns 3 documents with smallest distances
```

### In Your Project

```python
# src/retrieval/hybrid_search.py: Semantic search
response = collection.query(
    query_embeddings=embedding_function([user_query]),
    n_results=3,
    where=None  # Optional: filter by metadata
)

# ChromaDB returns:
{
    "ids": ["chunk_5", "chunk_12", "chunk_3"],
    "documents": [
        "Ronaldo scored 128 international goals",
        "Career records include 890 goals",
        "Goal-scoring statistics..."
    ],
    "distances": [0.05, 0.08, 0.12],  # Smaller = more similar
    "metadatas": [...]
}

# Store as RetrievedDocument (in src/retrieval/hybrid_search.py):
for doc, distance, metadata in zip(...):
    retrieved_docs.append(RetrievedDocument(
        content=doc,
        distance=distance,
        source=metadata["source"],
        source_type=metadata["source_type"]
    ))
```

**Advantages:**
âœ“ Understands synonyms ("goals" = "scoring")
âœ“ Finds conceptually related content
âœ“ Language-independent (embeddings capture meaning)

**Disadvantages:**
âœ— Can retrieve irrelevant but similar-sounding text
âœ— Computationally expensive (millions of comparisons)
âœ— Requires embedding model (adds overhead)

---

## 3.2 Keyword Search (BM25)

### What is BM25?

**BM** = Best Matching
**25** = Version 25 (mature algorithm)

**Definition:** Rank documents by how relevant keywords are.

```
Query: "Ronaldo goals"

Tokenize: ["ronaldo", "goals"]

Check each document:
  Doc1: "Ronaldo scored 128 goals"
    - "ronaldo": 1 match
    - "goals": 1 match
    - Score: 9.5/10 âœ“ HIGHLY RELEVANT

  Doc2: "The goals were achieved"
    - "ronaldo": 0 matches
    - "goals": 1 match
    - Score: 2.1/10 âœ— LOW RELEVANCE

Return docs sorted by score (highest first)
```

### BM25 Algorithm (Simplified)

```python
# Simplified BM25 formula (actual formula more complex)

def bm25_score(word_count, doc_length, avg_doc_length, total_docs):
    """
    Accounts for:
    1. How many times word appears (frequency)
    2. How long document is (longer docs â†’ less impact per word)
    3. How rare the word is (rare words â†’ more important)
    """
    # Pseudocode
    score = 0
    for query_word in query:
        freq_in_doc = word_count[query_word]
        word_rarity = log(total_docs / docs_with_word)

        # Combine: frequency Ã— rarity
        # But penalize frequency if doc is too long
        score += word_rarity * (freq_in_doc / (freq_in_doc + length_factor))

    return score
```

### In Your Project

```python
# src/retrieval/hybrid_search.py
from rank_bm25 import BM25Okapi

# HybridSearchEngine.keyword_search() method
def keyword_search(self, collection_name, query, top_k=3):
    # 1. Tokenize query
    query_tokens = self._tokenize(query)

    # 2. Get BM25 index for this collection
    bm25 = self.bm25_indices[collection_name]

    # 3. Calculate scores for all documents
    scores = bm25.get_scores(query_tokens)

    # 4. Sort by score and return top 3
    ranked = sorted(enumerate(zip(self.chunk_storage[collection_name], scores)))

    return ranked[:top_k]
```

**Advantages:**
âœ“ Fast (no vector calculations)
âœ“ Transparent (easy to debug)
âœ“ Effective for keyword-heavy queries
âœ“ Works in low-resource settings

**Disadvantages:**
âœ— Misses synonyms ("score" vs "goals")
âœ— Doesn't understand context
âœ— Fails on conceptual queries

---

## 3.3 Hybrid Search

### Why Mix Both?

```
Query: "How did Ronaldo achieve his records?"

Keyword Search (BM25):
âœ“ Finds: "Ronaldo records achievements"
âœ— Misses: "He reached his milestones"

Semantic Search:
âœ“ Finds: "Career milestones and achievements"
âœ— Misses: Might retrieve "Messi's records" (too similar)

Hybrid Search (Combine Both):
âœ“ Finds: Both exact matches AND semantic matches
âœ“ More robust, fewer false positives
```

### How Hybrid Search Works

```
1. RUN BOTH SEARCHES
   Semantic Results:
   â”œâ”€ Doc A: distance=0.05, relevance=0.95
   â”œâ”€ Doc B: distance=0.12, relevance=0.88
   â””â”€ Doc C: distance=0.20, relevance=0.80

   Keyword Results:
   â”œâ”€ Doc D: BM25_score=9.5, relevance=0.95
   â”œâ”€ Doc A: BM25_score=8.2, relevance=0.82
   â””â”€ Doc E: BM25_score=7.1, relevance=0.71

2. NORMALIZE SCORES (0-1 range)
   Each ranking system might score differently
   Normalize to 0-1 for fair comparison

3. WEIGHTED COMBINATION
   Hybrid_score = (semantic_score Ã— 0.7) + (keyword_score Ã— 0.3)

   In your project: 70% semantic + 30% keyword
   (Semantic more important because user queries often conceptual)

4. RANK COMBINED RESULTS
   Final ranking:
   â”œâ”€ Doc A: 0.95Ã—0.7 + 0.82Ã—0.3 = 0.91 âœ“ Won!
   â”œâ”€ Doc D: 0Ã—0.7 + 0.95Ã—0.3 = 0.29
   â”œâ”€ Doc B: 0.88Ã—0.7 + 0Ã—0.3 = 0.62
   â””â”€ Doc C: 0.80Ã—0.7 + 0Ã—0.3 = 0.56

   Return: [Doc A, Doc B, Doc C, Doc D, ...]
```

### In Your Project

```python
# src/retrieval/hybrid_search.py: Hybrid search implementation
def hybrid_search(self, query, semantic_results, keyword_results):
    combined = {}

    # Add semantic results
    semantic_scores = self.normalize_scores([score for _, score in semantic_results])
    for i, (doc, _) in enumerate(semantic_results):
        score = semantic_scores[i] * HYBRID_SEARCH_WEIGHT_SEMANTIC  # 0.7
        combined[doc] = combined.get(doc, 0) + score

    # Add keyword results
    keyword_scores = self.normalize_scores([score for _, score in keyword_results])
    for i, (doc, _) in enumerate(keyword_results):
        score = keyword_scores[i] * HYBRID_SEARCH_WEIGHT_KEYWORD  # 0.3
        combined[doc] = combined.get(doc, 0) + score

    # Return sorted by combined score
    return sorted(combined.items(), key=lambda x: x[1], reverse=True)

# Configure weights (in src/models/config.py):
HYBRID_SEARCH_WEIGHT_SEMANTIC = 0.7  # 70% importance
HYBRID_SEARCH_WEIGHT_KEYWORD = 0.3   # 30% importance
```

---

## 3.4 Query Expansion

### Definition

Automatically generate variations of user query for better coverage.

```
User Query: "What did Ronaldo achieve?"

Generated Variations:
â”œâ”€ "Ronaldo's achievements and records"
â”œâ”€ "What records did Cristiano Ronaldo break?"
â”œâ”€ "Career milestones of Ronaldo"
â””â”€ "Ronaldo's accomplishments in football"

Then search with ALL 4 queries
Retrieve union of results
Better coverage! âœ“
```

### Why Expansion Helps

```
Without expansion:
  Query: "What did Ronaldo achieve?"
  Search: Looks for "achieve" or similar semantic meaning
  Found: âœ“ Found "achievements"
  Missed: âœ— Missed "career records" (different phrasing)

With expansion:
  Variation: "What records did Ronaldo break?"
  Search: Looks for "records" or similar meaning
  Found: âœ“ Found "career records"
  Found: âœ“ Found "achievements" (from original)
  Coverage: Much better!
```

### Implementation

```python
# src/reasoning/query_expander.py: QueryExpander class

@staticmethod
def generate_variations(query: str, num_variations: int = 4):
    """Use LLM to generate alternative phrasings"""

    # Prompt the LLM to generate variations
    prompt = f"""Generate {num_variations} alternative phrasings
    for this query from different angles...

    Original: {query}"""

    # LLM generates variations
    response = client.chat.completions.create(
        model=OPEN_AI_MODEL,
        messages=[{"role": "user", "content": prompt}],
        temperature=0.7  # Higher temperature = more creative
    )

    # Parse response, return list
    variations = parse_response(response)
    return [query] + variations[:num_variations-1]  # Include original
```

### In RAG Pipeline

```python
# src/core/__init__.py: process_query_with_expansion

def process_query_with_expansion(user_query, num_expansions=4):
    # 1. Generate variations
    variations = QueryExpander.generate_variations(user_query, num_expansions)

    # 2. Retrieve for each variation
    all_docs = {}
    for variant in variations:
        docs, _ = self._retrieve_relevant_chunks(variant)
        for doc in docs:
            all_docs[doc.content] = doc  # Deduplicate by content

    # 3. Return union of all results
    retrieved_docs = list(all_docs.values())

    # 4. Generate single answer using all docs
    answer = self._generate_answer(user_query, retrieved_docs)

    return answer
```

**When to Use:**
âœ“ Complex queries ("Compare X and Y considering Z")
âœ“ Ambiguous queries ("Tell me about design")
âœ“ Non-English queries (translate to English variations)

**When NOT to Use:**
âœ— Simple, clear queries (wastes computation)
âœ— Time-sensitive applications (too slow)
âœ— Cost-conscious projects (uses LLM call for each variation)

---

## 3.5 Multi-hop Reasoning

### Definition

Break complex queries into steps, fetch information for each step, synthesize final answer.

```
Complex Query: "How does Ronaldo's record compare to Messi's?"

Without Multi-hop:
  Try to find documents about both at once
  âœ— Rarely finds good comparisons

With Multi-hop:
  Step 1: "What are Ronaldo's records?"
    â†’ Retrieve Ronaldo docs
  â†“
  Step 2: "What are Messi's records?"
    â†’ Retrieve Messi docs
  â†“
  Step 3: "Compare these records"
    â†’ Synthesize comparison
  âœ“ Much better!
```

### Why It Works

```
Human reasoning is multi-hop:
  Q: "Who won the championship in year X?"

  Step 1: What teams played in year X?
  Step 2: Who won the tournament?
  Step 3: Return winner

AI should mirror this!
```

### Implementation

```python
# src/reasoning/multi_hop_reasoner.py: MultiHopReasoner class

def decompose_query(query: str, max_steps: int = 3):
    """Break query into sub-questions"""

    prompt = f"""Break down this query into {max_steps}
    simpler sub-questions:
    {query}"""

    # LLM decomposes â€” temperature=0.5 (some creativity to generate diverse sub-questions)
    response = client.chat.completions.create(
        model=model_name,
        messages=[{"role": "user", "content": prompt}],
        temperature=0.5,   # â† Higher than generation; allows varied decomposition angles
    )

    # Returns: ["Sub Q1", "Sub Q2", "Sub Q3"]
    return response

def synthesize_answer(query: str, step_results: List[Dict]):
    """Combine step answers into final answer"""

    prompt = f"""Based on these step-by-step results,
    answer the original query: {query}

    Step results: {step_results}"""

    # temperature=0.3 â€” more factual than decomposition; synthesis must stay grounded
    response = client.chat.completions.create(
        model=model_name,
        messages=[{"role": "user", "content": prompt}],
        temperature=0.3,   # â† Lower than decomposition; synthesis should be faithful
    )

    return response.content
```

### Temperature Strategy in Multi-Hop

```
DECOMPOSE  temperature=0.5
  - Needs creativity to explore different angles of a complex question
  - Too low â†’ sub-questions are too similar (no coverage gain)
  - Too high â†’ nonsensical sub-questions

SYNTHESIZE temperature=0.3
  - Must stay close to retrieved evidence
  - Slightly higher than base generation (0.2) to flow step results into a coherent answer
  - Too high â†’ drifts from evidence (hallucination risk)
```

### Pipeline Diagram

```
User Query: "How did Ronaldo become the greatest?"
        â†“
    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
    â”‚ DECOMPOSE             â”‚
    â”‚ (LLM breaks into 3)   â”‚
    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
        â†“
    Step 1: "When did Ronaldo start his career?"
    â””â”€â†’ RETRIEVE â†’ synthesize answer
        â†“
    Step 2: "What records did he break?"
    â””â”€â†’ RETRIEVE â†’ synthesize answer
        â†“
    Step 3: "How many goals did he score?"
    â””â”€â†’ RETRIEVE â†’ synthesize answer
        â†“
    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
    â”‚ SYNTHESIZE            â”‚
    â”‚ (Combine all steps)   â”‚
    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
        â†“
    Final Comprehensive Answer
```

### Code in Your Project

```python
# src/reasoning/multi_hop_reasoner.py and src/core/__init__.py

def process_query_multihop(user_query, max_steps=3):
    # Step 1: Decompose
    subqueries = MultiHopReasoner.decompose_query(user_query, max_steps)

    step_results = []
    for i, subquery in enumerate(subqueries):
        # Step 2: Retrieve for this subquery
        docs = self._retrieve_relevant_chunks(subquery)

        # Step 3: Generate answer for this step
        answer = self._generate_answer(subquery, docs)

        # Store result
        step_results.append({
            "step": i+1,
            "subquery": subquery,
            "answer": answer
        })

    # Step 4: Synthesize final answer
    final_answer = MultiHopReasoner.synthesize_answer(
        user_query,
        step_results
    )

    return final_answer
```

---

# PART 4: AI/ML & EVALUATION

> **What you'll learn:** How to measure answer quality with RAGAS (three distinct metrics), how hallucinations are detected at the claim level, how the system auto-mitigates HIGH-risk answers, and how confidence scoring works.
>
> **Prerequisites:** PART 3 (especially Â§3.1 and Â§3.3 for context on what is being evaluated).
>
> **Time:** 90 min
>
> **After this part:** Enable all evaluation toggles and inspect output:
> ```
> > hallucination
> > fact-check
> > query What is reinforcement learning?
> > metrics
> > hallucination-report
> > facts
> ```
>
> **Key source files:** `src/evaluation/ragas_evaluator.py`, `src/evaluation/hallucination_detector.py`, `src/evaluation/fact_checker.py`

## 4.1 RAGAS Metrics

### What is RAGAS?

**RAGAS** = Retrieval Augmented Generation Assessment

A framework to measure how well your RAG system works. Think of it like unit tests for AI.

```
Code Testing:               AI Testing (RAGAS):
â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•      â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
assert response == 200     assert context_relevance > 0.8
assert len(data) > 0       assert faithfulness > 0.9
test_function()            evaluate_rag_pipeline()
```

### The Three Metrics

#### 1. Context Relevance (CR)

**Question:** Are the retrieved documents relevant to the query?

```
Scenario 1 (Bad):
  Query: "How many goals did Ronaldo score?"
  Retrieved Doc: "Ronaldo was born in 1985"
  Relevance: âŒ 0.1/1.0 (Not relevant)

Scenario 2 (Good):
  Query: "How many goals did Ronaldo score?"
  Retrieved Doc: "Ronaldo scored 128 international goals"
  Relevance: âœ… 0.95/1.0 (Highly relevant)
```

**Formula (Simplified):**
```
Context Relevance = LLM_Score(Is document relevant to query?)
                  = Human judgment of relevance
```

**In Code (src/evaluation/ragas_evaluator.py):**
```python
def evaluate_context_relevance(query: str, context: str):
    prompt = f"""Query: {query}
    Context: {context[:500]}

    On scale 0-10, how relevant is this context?"""

    # LLM is the judge
    score = client.chat.completions.create(...)
    return score / 10.0  # Convert to 0-1
```

**Why This Matters:**
If you retrieve irrelevant documents, the LLM wastes time processing garbage.

#### 2. Answer Relevance (AR)

**Question:** Does the generated answer actually address the query?

```
Scenario 1 (Bad):
  Query: "How many goals?"
  Answer: "Ronaldo is known for his work ethic"
  Relevance: âŒ 0.2/1.0 (Doesn't answer question)

Scenario 2 (Good):
  Query: "How many goals?"
  Answer: "Ronaldo scored 128 international goals"
  Relevance: âœ… 0.99/1.0 (Directly answers)
```

**In Code (src/evaluation/ragas_evaluator.py):**
```python
def evaluate_answer_relevance(query: str, answer: str):
    prompt = f"""Query: {query}
    Answer: {answer[:500]}

    On scale 0-10, how well does this answer address the query?"""

    score = client.chat.completions.create(...)
    return score / 10.0
```

#### 3. Faithfulness (F)

**Question:** Is the answer grounded in the provided context? (No hallucinations?)

```
Scenario 1 (Bad - Hallucination):
  Context: "Ronaldo plays for Al Nassr"
  Answer: "Ronaldo recently returned to Manchester United"
  Faithfulness: âŒ 0.1/1.0 (Made up!) â† HALLUCINATION

Scenario 2 (Good - Grounded):
  Context: "Ronaldo plays for Al Nassr"
  Answer: "Ronaldo plays in Saudi Arabia"
  Faithfulness: âœ… 0.95/1.0 (Matches context)

Scenario 3 (Okay - Simplification):
  Context: "Ronaldo has 128 international goals over 20 years"
  Answer: "Ronaldo has many international goals"
  Faithfulness: âš ï¸ 0.7/1.0 (True but simplified)
```

**In Code (src/evaluation/ragas_evaluator.py):**
```python
def evaluate_faithfulness(context: str, answer: str):
    prompt = f"""Context: {context[:500]}
    Answer: {answer[:500]}

    On scale 0-10, how much of the answer is supported by context?"""

    score = client.chat.completions.create(...)
    return score / 10.0
```

**Why This is Critical:**
This detects hallucinations - the biggest problem with LLMs!

### Computing RAG Score

```python
# src/evaluation/ragas_evaluator.py:

def compute_rag_score(context_relevance, answer_relevance, faithfulness):
    weights = [0.30, 0.35, 0.35]
    scores = [context_relevance, answer_relevance, faithfulness]

    # Weighted average
    rag_score = sum(s * w for s, w in zip(scores, weights))
    return rag_score

# Example:
context_rel = 0.90
answer_rel = 0.85
faithful = 0.95

rag_score = (0.90 Ã— 0.30) + (0.85 Ã— 0.35) + (0.95 Ã— 0.35)
          = 0.27 + 0.2975 + 0.3325
          = 0.90

# 90% RAG Quality âœ“ Excellent!
```

### Weight Explanation

```
Context (30%):    Retrieved docs matter, but...
Answer (35%):     Answering the question matters more
Faithfulness (35%): Not hallucinating is equally important

Why these weights?
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
A: Bad context + Good answer = Some value (retrieved something useful)
B: Good context + Bad answer = Some value (had data, but didn't use it)
C: Good context + Good answer + Hallucination = WORTHLESS (false info)

Faithfulness must be HIGH â†’ Set equal to answer relevance
```

### Using RAGAS in Your Project

```python
# Lines 1278-1320: process_query pipeline

def process_query(user_query, enable_evaluation=True):
    # 1. Retrieve documents
    retrieved_docs = self._retrieve_relevant_chunks(user_query)

    # 2. Generate answer
    answer = self._generate_answer(user_query, retrieved_docs)

    # 3. Build context
    context = "\n".join([doc.content for doc in retrieved_docs])

    # 4. EVALUATE (NEW)
    if enable_evaluation:
        rag_metrics = self.evaluator.evaluate(
            query=user_query,
            context=context,
            answer=answer
        )
        # rag_metrics.rag_score = 0-1
        # Store for analysis
        self.evaluation_results.append(rag_metrics)

    return answer, rag_metrics
```

---

## 4.2 Hallucination Detection

### What are Hallucinations?

**Definition:** LLM generates plausible-sounding but false information.

```
Context: "Ronaldo plays for Al Nassr since 2023"
Query: "Which team does Ronaldo play for?"
LLM Output: "Ronaldo plays for Liverpool"

âŒ HALLUCINATION - Liverpool is completely false!
```

### Why LLMs Hallucinate

```
LLM Training Process:
  1. Trained on internet data (includes false information)
  2. Learning: Predict next word based on patterns
  3. LLMs are "pattern matching machines", not fact databases

Example:
  LLM sees in training: "Ronaldo plays for..."
  Common next words: "Manchester United" (from old Wikipedia)
  LLM might predict this even if outdated

Result: Plausible-sounding but wrong
```

### Detection Strategies

#### 1. Faithfulness Metric (RAGAS)
```python
# Does answer match the context?
faithfulness = evaluate_faithfulness(context, answer)

if faithfulness < 0.5:
    print("âš ï¸ Likely hallucinating!")
```

#### 2. Source Attribution
```python
# In your project (lines 1499-1509):
def print_response(self, response):
    # Show which source each fact comes from
    for source in response.sources:
        print(f"[Source: {source}]")
        print(doc.content)

# If LLM says something not in sources â†’ Hallucination detected
```

#### 3. Confidence Scoring
```python
# Return confidence = average relevance of sources
avg_confidence = sum(doc.relevance_score for doc in docs) / len(docs)

if avg_confidence < 0.6:
    print("âš ï¸ Low confidence - might be hallucinating")
```

### In Your Project

```python
# System prompt prevents hallucination (lines 1044-1050):

system_prompt = """You are a knowledgeable assistant...
Guidelines:
- Only use information from the provided context â† CRITICAL
- If answer not in context, state: "I don't have information..."
- Cite which source you're using
- Be precise and concise"""

# Temperature = 0.2 (very factual, not creative)
# Creative = More likely to hallucinate
```

### Advanced: Claim-Level Hallucination Detection

The system implements a dedicated `HallucinationDetector` (`src/evaluation/hallucination_detector.py`) that goes beyond RAGAS by analyzing every individual factual claim in the generated answer.

#### How It Works

```
Answer: "Ronaldo joined Real Madrid in 2009 and scored 450 goals over 9 seasons."

STEP 1 â€” Extract claims (up to _MAX_CLAIMS=8):
  Claim 1: "Ronaldo joined Real Madrid in 2009"
  Claim 2: "scored 450 goals over 9 seasons"

STEP 2 â€” Grade each claim against retrieved context chunks (up to _MAX_SOURCES=3):
  For each claim, LLM returns a grounding score 0â€“10
    0  = Claim contradicts context
    5  = Claim is plausible but not directly supported
    10 = Claim is explicitly confirmed by context

  Claim 1 score: 9   (Wikipedia says "2009 transfer")
  Claim 2 score: 3   (context says "450+ goals" but period is unclear)

STEP 3 â€” Aggregate:
  Normalise scores to 0â€“1 (divide by 10)
  grounding_score = mean([0.9, 0.3]) = 0.60

STEP 4 â€” Risk classification:
  grounding_score >= _HIGH_THRESHOLD (0.50)   â†’ LOW risk
  _MEDIUM_THRESHOLD (0.25) <= score < 0.50    â†’ MEDIUM risk
  grounding_score <  _MEDIUM_THRESHOLD        â†’ HIGH risk

  0.60 >= 0.50 â†’ LOW risk (answer is reasonably grounded)
```

#### Risk Thresholds

```python
# src/evaluation/hallucination_detector.py
_HIGH_THRESHOLD   = 0.50   # below this â†’ HIGH risk
_MEDIUM_THRESHOLD = 0.25   # below this â†’ MEDIUM risk (between 0.25â€“0.50 â†’ MEDIUM)

# Counter-intuitive naming: _HIGH_THRESHOLD is the bar for LOW risk
# Think of it as: "you need this high a score to be LOW risk"
```

#### Auto-Mitigation

When risk is HIGH and `auto_mitigate=True`, the system regenerates the answer with a stricter prompt:

```python
def _mitigate(self, query, answer, context):
    """Regenerate with stronger grounding instructions"""
    prompt = f"""The following answer may contain unsupported claims.
Rewrite it using ONLY information explicitly found in the context below.
If a claim cannot be verified, omit it or flag it as uncertain.

CONTEXT:
{context}

ORIGINAL ANSWER:
{answer}"""

    # temperature=0.2 â€” factual, conservative regeneration
    response = client.chat.completions.create(temperature=0.2, ...)
    return response.content
```

#### Key Design Choices

```
_MAX_CLAIMS = 8   â€” Cap to avoid excessive LLM calls on long answers
_MAX_SOURCES = 3  â€” Use top-3 retrieved chunks only (diminishing returns beyond 3)
temperature = 0.0 â€” Grounding judgement calls must be deterministic, not creative
```

---

## 4.3 Confidence Scoring

### Definition

A score (0-1) representing how confident the system is in its answer.

```
Low Confidence (0.3):
  "I'm not very sure, but maybe Ronaldo scored around 100 goals?"

High Confidence (0.9):
  "Ronaldo scored 128 international goals (from Wikipedia)"

User can use this to decide: Trust this answer or search elsewhere?
```

### How to Calculate

```python
# Simple approach (lines 1067):

# Average relevance of retrieved documents
avg_confidence = sum(doc.relevance_score for doc in context_docs) / len(context_docs)

# Logic:
# If documents are very relevant (high relevance_score)
# â†’ Answer is probably good (high confidence)
# If documents are barely relevant (low relevance_score)
# â†’ Answer might be wrong (low confidence)
```

### More Sophisticated Approach

```python
# Could also consider:

def compute_confidence(
    retrieval_quality,      # How good were docs? (0-1)
    answer_relevance,       # Does answer match query? (0-1)
    faithfulness,           # Grounded in context? (0-1)
    doc_count               # How many sources? (more = better)
):
    # Combine multiple signals
    confidence = (
        retrieval_quality * 0.4 +
        answer_relevance * 0.3 +
        faithfulness * 0.3
    )

    # Bonus for multiple sources
    if doc_count >= 3:
        confidence *= 1.05  # 5% boost

    # Cap at 1.0
    return min(1.0, confidence)
```

### Display in Your Project

```python
# Store in conversation history (lines 1293):
self.conversation_history.append(ConversationMessage(
    role="assistant",
    content=answer,
    confidence_score=confidence,  # â† Stored
    sources=[...]
))

# Show to user (lines 1544):
print(f"Confidence Score: {response.confidence_score:.1%}")
# Output: "Confidence Score: 87%"
```

---

## 4.4 Adversarial Testing

### Definition

Deliberately give the system hard/weird questions to find weaknesses.

```
Normal Testing:
  Q: "Who is Ronaldo?"
  A: Works fine âœ“

Adversarial Testing:
  Q: "What color is number 7?"           â† Invalid question
  Q: "" (empty)                           â† Edge case
  Q: "!@#$%^&*()"                        â† Special chars
  How does system handle failures?
```

### Test Categories

#### 1. Ambiguous Queries

```python
# Lines 711-718:
AdversarialTestCase(
    test_id="ambig_001",
    query="What about design?",
    test_type="ambiguous",
    expected_behavior="Ask for clarification or give multiple options"
)

# System behavior:
Q: "What about design?"
A: "I need more context. Design of what?
    - Graphic design?
    - UI design?
    - Game design?"
```

#### 2. No Valid Answer

```python
# Lines 723-732:
AdversarialTestCase(
    test_id="noans_001",
    query="What color is number 7?",
    test_type="no_answer",
    expected_behavior="Acknowledge question is unanswerable"
)

# System behavior:
Q: "What color is number 7?"
A: "Numbers don't have colors. This question doesn't make sense."
```

#### 3. Edge Cases

```python
# Lines 737-751:
AdversarialTestCase(
    test_id="edge_001",
    query="",  # Empty
    test_type="edge_case",
    expected_behavior="Handle gracefully"
)

AdversarialTestCase(
    test_id="edge_002",
    query="a" * 1000,  # Very long
    test_type="edge_case",
    expected_behavior="Truncate or reject gracefully"
)
```

### Running Tests (Your Project)

```python
# Lines 760-774: run_all_tests method

def run_all_tests(rag_system):
    test_cases = AdversarialTestSuite.generate_test_cases()
    results = []

    for test_case in test_cases:
        result = AdversarialTestSuite.run_test_case(rag_system, test_case)
        results.append(result)

    return results

# Usage: Type 'test' in interactive mode
```

### Results Analysis

```
Output:
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ Test ID    â”‚ Type         â”‚ Query            â”‚ Result â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ ambig_001  â”‚ ambiguous    â”‚ What about...?   â”‚ âœ… PASSâ”‚
â”‚ noans_001  â”‚ no_answer    â”‚ What color...?   â”‚ âœ… PASSâ”‚
â”‚ edge_001   â”‚ edge_case    â”‚ (empty)          â”‚ âŒ FAILâ”‚
â”‚ edge_002   â”‚ edge_case    â”‚ aaaa... (1000)   â”‚ âœ… PASSâ”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”˜

Total: 4 tests, 3 passed (75%)
Failures: edge_001 (empty query) - System crashed
```

---

# PART 5: DATA PROCESSING

> **What you'll learn:** Why chunking matters and how chunk size affects retrieval quality, how the adaptive chunker detects content type (academic/structured/general) and picks the right size, and how the system loads from multiple source types (Wikipedia, URLs, PDFs, local files).
>
> **Prerequisites:** PART 2 (ChromaDB context helps here).
>
> **Time:** 45 min
>
> **After this part:** Open `src/retrieval/chunker.py` and find the three chunk size configurations â€” you now understand exactly why each value was chosen.
>
> **Key source files:** `src/retrieval/chunker.py`, `src/retrieval/loader.py`

## 5.1 Adaptive Chunking

### The Chunking Problem

```
Document: "Ronaldo played at Manchester United for 6 years..."
(Suppose it's 10,000 words)

Option 1: Store as 1 chunk
â”œâ”€ Pro: Context intact
â””â”€ Con: Too long, hard to find specific information

Option 2: Split every 100 words
â”œâ”€ Pro: Manageable size
â””â”€ Con: Might break important concepts

Option 3: Adaptive chunking
â”œâ”€ Pro: Smart sizing based on content type
â””â”€ Pro: Keeps concepts together
```

### Your Adaptive Chunking Approach

```python
# Lines 228-276: AdaptiveChunker class

def detect_content_type(text):
    """Analyze content to determine optimal chunk size"""

    # Heuristic 1: Check average line length
    avg_line_length = sum(len(line) for line in text.split('\n')) / len(text.split('\n'))

    # Heuristic 2: Count academic keywords
    academic_keywords = ['research', 'study', 'analysis', 'methodology', ...]
    academic_count = sum(1 for kw in academic_keywords if kw.lower() in text[:500])

    # Classify
    if academic_count >= 2:
        return 'academic'
    elif avg_line_length < 60:
        return 'structured'  # Code, lists
    else:
        return 'general'  # News, articles

def get_optimal_chunk_size(content_type):
    """Return (chunk_size, overlap) for each type"""

    configs = {
        'academic': (1024, 200),     # Large with good overlap
        'structured': (256, 32),     # Small, minimal overlap
        'general': (512, 50)         # Medium
    }

    return configs[content_type]
```

### Why Overlap?

```
Chunk 1: "Ronaldo joined Manchester United in 2003. He played there
         for six seasons, scoring 84 goals. At United..."

Chunk 2: "At United, he won 3 Premier League titles. He scored 84
         goals total. After leaving, he moved to Real Madrid..."

Overlap ensures:
- Key concepts not cut off mid-sentence
- Context preserved across chunks
- Better retrieval (query might match overlap region)
```

### In Your Project

```python
# Lines 905-913: Load and chunk document

content = load_from_wikipedia("Cristiano Ronaldo")

# Automatically chunk with adaptive sizing
chunks = AdaptiveChunker.adaptive_chunk(content)

# Log output:
# ğŸ” Detected content type: general (chunk_size=500, overlap=100)
# âœ… Successfully loaded 42 chunks from Cristiano Ronaldo
```

---

## 5.2 Multi-source Loading

### Sources Your System Supports

```
1. Wikipedia (lines 790-806):
   â”œâ”€ Pros: Structured, well-edited, free
   â””â”€ Cons: General purpose, might lack domain specifics

2. URLs/Web Pages (lines 790-788):
   â”œâ”€ Pros: Real-time, current information
   â””â”€ Cons: Inconsistent formatting, content extraction hard

3. Local Files (lines 808-825):
   â”œâ”€ Pros: Private data, full control
   â””â”€ Cons: Manual maintenance
```

### Detection

```python
# Lines 838-850: detect_source_type

def detect_source_type(source: str) -> str:
    """Automatically identify source type"""

    if source.startswith(('http://', 'https://')):
        return 'url'
    elif source.endswith(('.txt', '.md', '.pdf')):
        return 'file'
    else:
        return 'wikipedia'

# Usage
source = "Cristiano Ronaldo"
source_type = detect_source_type(source)  # Returns: 'wikipedia'

source = "https://example.com/article"
source_type = detect_source_type(source)  # Returns: 'url'

source = "local_document.txt"
source_type = detect_source_type(source)  # Returns: 'file'
```

### Web Scraping (BeautifulSoup)

```python
# Lines 790-808: scrape_url method

def scrape_url(url: str) -> str:
    """Extract text content from webpage"""

    # 1. Fetch HTML
    response = requests.get(url, headers=headers, timeout=10)

    # 2. Parse HTML
    soup = BeautifulSoup(response.content, 'html.parser')

    # 3. Remove noise (script, style tags)
    for script in soup(["script", "style"]):
        script.decompose()

    # 4. Extract clean text
    text = soup.get_text()
    text = '\n'.join(line.strip() for line in text.split('\n') if line.strip())

    return text

# Example:
url = "https://en.wikipedia.org/wiki/Cristiano_Ronaldo"
content = scrape_url(url)
# Returns: "Cristiano Ronaldo is a Portuguese professional footballer..."
```

### Wikipedia API

```python
# Lines 259-260: Initialize Wikipedia API

from wikipediaapi import Wikipedia
USER_AGENT = "generative-ai-learning/1.0"
wiki = Wikipedia(user_agent=USER_AGENT, language="en")

# Lines 794-804: Load Wikipedia page

def load_wikipedia_page(page_name: str) -> str:
    """Fetch Wikipedia article"""

    page = wiki.page(page_name)

    # Check if page exists
    if not page.exists():
        return None

    # Return article text
    return page.text

# Usage
content = load_wikipedia_page("Cristiano Ronaldo")
# Returns: Full Wikipedia article text
```

### Storage in ChromaDB

```python
# Lines 908-923: Store with metadata

collection.add(
    ids=[f"chunk_{i}" for i in range(len(chunks))],
    documents=chunks,
    metadatas=[  # Track source info
        {
            "source": source,           # Where from
            "source_type": source_type, # Type: wikipedia/url/file
            "index": i,                 # Position in chunking
            "timestamp": datetime.now().isoformat()
        }
        for i in range(len(chunks))
    ]
)
```

---

## 5.3 Text Processing (NLTK)

### Tokenization

```python
# Lines 22-23: Import NLTK
from nltk.tokenize import word_tokenize
from nltk.corpus import stopwords

# Lines 26-36: Download required data
nltk.download('punkt', quiet=True)  # Tokenizer
nltk.download('stopwords', quiet=True)  # Common words
```

### What is a Token?

```
Sentence: "Ronaldo scored 128 goals in his career."

Tokenization (split into words):
â”œâ”€ "Ronaldo"
â”œâ”€ "scored"
â”œâ”€ "128"
â”œâ”€ "goals"
â”œâ”€ "in"
â”œâ”€ "his"
â”œâ”€ "career"
â””â”€ "."

Tokens = [Ronaldo, scored, 128, goals, in, his, career, .]
```

### Stop Words

```python
# Common words that don't add meaning
stopwords = {"the", "a", "is", "in", "and", "or", ...}

Sentence: "The cat is in the house"
All tokens: ["The", "cat", "is", "in", "the", "house"]

Remove stopwords:
Result: ["cat", "house"]

Why remove?
- "the" appears in nearly every document â†’ useless for similarity
- Reduces noise in keyword search (BM25)
```

### In Your Project

```python
# Lines 290-305: _tokenize method in HybridSearchEngine

def _tokenize(self, text: str) -> List[str]:
    """Tokenize and preprocess text"""

    # 1. Lowercase
    tokens = word_tokenize(text.lower())

    # 2. Keep only alphanumeric
    # 3. Remove stopwords
    tokens = [token for token in tokens
              if token.isalnum() and token not in self.stop_words]

    return tokens

# Usage
query = "How many goals did Ronaldo score?"
tokens = self._tokenize(query)
# Result: ["many", "goals", "ronaldo", "score"]
# Removed: "How", "did"
```

---

# PART 6: ENGINEERING PATTERNS

> **What you'll learn:** Three production engineering concerns that distinguish a demo from a real system: conversation memory (context-aware follow-up questions), source attribution (knowing where each answer came from), and observability (logging and metrics for debugging and monitoring).
>
> **Prerequisites:** Parts 1â€“4 (you need context on what is being remembered, attributed, and observed).
>
> **Time:** 45 min
>
> **After this part:** Run `history` to see conversation context, then read `src/core/rag_system.py` â€” you will recognise every pattern.
>
> **Key source files:** `src/core/rag_system.py`, `src/persistence/__init__.py`, `src/utils/__init__.py`

## 6.1 Conversation Memory

### Why Conversation Memory?

```
Without Memory:
  User: "Who is Ronaldo?"
  System: "Cristiano Ronaldo is a footballer from Portugal"

  User: "How many goals?"
  System: "âš ï¸ ERROR: Who is 'he'? Need clarification"
  âœ— Context lost!

With Memory:
  User: "Who is Ronaldo?"
  System: "Cristiano Ronaldo is a footballer from Portugal"
  [Stored in memory]

  User: "How many goals?"
  System: "Ronaldo scored 128 international goals"
  âœ“ Understands "he" = Ronaldo from previous message
```

### Implementation

```python
# Lines 885-887: Store conversation

self.conversation_history: List[ConversationMessage] = []

# Lines 88-97: ConversationMessage data structure

@dataclass
class ConversationMessage:
    role: str                          # "user" or "assistant"
    content: str                       # Message text
    timestamp: str                     # When?
    sources: Optional[List[Dict]] = None  # Where info from?
    confidence_score: Optional[float] = None  # How confident?
```

### Adding to Memory

```python
# Lines 1283-1293: Store query and answer

self.conversation_history.append(ConversationMessage(
    role="user",
    content=user_query,
    timestamp=datetime.now().isoformat(),
    sources=[{"source": doc.source, "type": doc.source_type}
             for doc in retrieved_docs]
))

self.conversation_history.append(ConversationMessage(
    role="assistant",
    content=answer,
    timestamp=datetime.now().isoformat(),
    confidence_score=confidence,
    sources=[{"source": doc.source, "type": doc.source_type}
             for doc in retrieved_docs]
))

# Save to file (persistence)
self._save_conversation_history()
```

### Using Memory in Answer Generation

```python
# Lines 1041-1047: Build context from history

def _build_conversation_context(self, max_messages: int = 4):
    """Get last 4 messages as context"""

    if not self.conversation_history:
        return ""

    recent_messages = self.conversation_history[-max_messages:]
    context = ""

    for msg in recent_messages:
        role = "User" if msg.role == "user" else "Assistant"
        context += f"{role}: {msg.content[:150]}...\n"

    return context

# Usage in prompt (lines 1055-1063):
user_content = f"""Previous Conversation Context:
{conv_context}

Retrieved Context from Sources:
{context}

User Question: {query}

Please provide a clear answer..."""
```

### Persistence

```python
# Lines 1385-1401: Save to JSON file

def _save_conversation_history(self):
    history_data = {
        "conversation_id": self.conversation_id,
        "timestamp": datetime.now().isoformat(),
        "messages": [msg.to_dict() for msg in self.conversation_history]
    }

    with open(CONVERSATION_HISTORY_FILE, 'w') as f:
        json.dump(history_data, f, indent=2)

# File: conversation_history.json
{
  "conversation_id": "20260225_103000",
  "timestamp": "2026-02-25T10:30:00",
  "messages": [
    {
      "role": "user",
      "content": "Who is Ronaldo?",
      "timestamp": "2026-02-25T10:30:05",
      "sources": [{"source": "Wikipedia", "type": "wikipedia"}]
    },
    ...
  ]
}
```

---

## 6.2 Source Attribution

### Why Track Sources?

```
Without Attribution:
  "Ronaldo scored 128 goals"
  (Where did this come from? ğŸ¤”)

With Attribution:
  "Ronaldo scored 128 goals [Source: Wikipedia, Index: 5]"
  (I know where this fact comes from âœ“)

User can:
- Verify the source
- Cross-check information
- Judge credibility
```

### Implementation

```python
# Lines 103-109: RetrievedDocument includes source

@dataclass
class RetrievedDocument:
    content: str
    source: str           # WHERE it came from
    source_type: str      # TYPE: 'wikipedia', 'url', 'file'
    index: int            # WHICH chunk number
    distance: Optional[float] = None  # Relevance

# Lines 911-918: Store source metadata

metadatas=[
    {
        "source": source,          # e.g., "Cristiano Ronaldo"
        "source_type": source_type,# e.g., "wikipedia"
        "index": i,                # e.g., 5
        "timestamp": datetime.now().isoformat()
    }
    for i in range(len(chunks))
]

# Retrieve (lines 995-1007):
for doc, distance, metadata in zip(documents, distances, metadatas):
    retrieved_docs.append(RetrievedDocument(
        content=doc,
        source=metadata["source"],
        source_type=metadata["source_type"],
        index=metadata["index"],
        distance=distance
    ))
```

### Display to User

```python
# Lines 1521-1534: Print sources

def print_response(self, response):
    if response.sources:
        print("\nğŸ“š SOURCES & CONTEXT")
        for i, doc in enumerate(response.sources, 1):
            print(f"\n[Source {i} - {doc.source_type.upper()}]")
            print(f"Source: {doc.source}")
            print(f"Content: {doc.content[:300]}...")

# Output:
# [Source 1 - WIKIPEDIA]
# Source: Cristiano Ronaldo
# Content: Ronaldo scored 128 international goals...
#
# [Source 2 - WIKIPEDIA]
# Source: Cristiano Ronaldo
# Content: His record includes 5 Ballon d'Or awards...
```

---

## 6.3 Observable Metrics & Logging

### Why Observability?

```
Production system without logging:
  "The system crashed"
  (What went wrong? Don't know. ğŸ¤·)

Production system with logging:
  "ğŸš€ Query received: 'How many goals?'
   ğŸ” Retrieved 3 chunks from Wikipedia
   ğŸ¤– Answer generated with 0.87 confidence
   ğŸ“Š RAGAS score: 0.89 (good)
   âœ… Response sent to user"
  (Entire flow visible. Easy to debug. âœ“)
```

### Logging Setup

```python
# Lines 54-56: Configure logging

logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s'
)
logger = logging.getLogger(__name__)
```

### Logging Levels

```
DEBUG (most verbose):
  "Tokenizing input text..."
  (Too much info for normal use)

INFO (what we use):
  "ğŸš€ Processing query"
  "ğŸ” Retrieved 3 chunks"
  "âœ… Response sent"
  (Useful without overwhelming)

WARNING:
  "âš ï¸ Low confidence (0.45)"
  (Something might be wrong)

ERROR (most severe):
  "âŒ Failed to reach LLM"
  (Something definitely wrong)

CRITICAL:
  "ğŸ”¥ Database failure"
  (System broken, immediate attention needed)
```

### Throughout Your Code

```python
# Line 1277: Process start
logger.info("=" * 80)
logger.info(f"ğŸš€ Processing query: {user_query[:60]}...")

# Line 1286: Retrieval
logger.info(f"ğŸ” Retrieving chunks...")
logger.info(f"âœ… Retrieved {len(retrieved_docs)} relevant chunks")

# Line 1289: Generation
logger.info(f"ğŸ¤– Generating answer...")

# Line 1309: Evaluation
logger.info("ğŸ“Š Running RAGAS evaluation...")
logger.info(f"âœ… Evaluation complete:\n{rag_metrics}")

# Line 1517: Completion
logger.info("=" * 80 + "\n")
```

### Metrics Saved

```python
# Lines 1413-1426: Save evaluation metrics

def _save_evaluation_metrics(self):
    metrics_data = {
        "conversation_id": self.conversation_id,
        "timestamp": datetime.now().isoformat(),
        "evaluations": [
            {
                "query": "What achievements...",
                "metrics": {
                    "context_relevance": 0.85,
                    "answer_relevance": 0.92,
                    "faithfulness": 0.88,
                    "rag_score": 0.88
                },
                "retrieval_method": "hybrid",
                "num_chunks": 3,
                "timestamp": "2026-02-25T10:30:05"
            }
        ]
    }

# File: evaluation_metrics.json
```

---

# PART 7: THE COMPLETE RAG PIPELINE

> **What you'll learn:** How all the components from Parts 1â€“6 connect into a single end-to-end flow. Read this as a synthesis, not new material â€” every step points back to a section you have already read.
>
> **Prerequisites:** All of Parts 1â€“6.
>
> **Time:** 30 min
>
> **After this part:** Read [WORKFLOWS.md](./WORKFLOWS.md) Â§2 Standard Query â€” it shows the same pipeline in operational detail (exact function calls and branching logic).

## End-to-End Flow

```
USER INPUT
    â†“
    â”œâ”€ ğŸ“– LOAD SOURCES (if needed)
    â”‚  â”œâ”€ Detect source type (Wikipedia/URL/File)
    â”‚  â”œâ”€ Fetch content
    â”‚  â”œâ”€ Adaptive chunking (content-aware sizing)
    â”‚  â”œâ”€ Convert to embeddings (semantic meaning)
    â”‚  â””â”€ Store in ChromaDB (with BM25 index)
    â”‚
    â”œâ”€ ğŸ” RETRIEVE RELEVANT DOCUMENTS
    â”‚  â”œâ”€ Semantic search (embeddings)
    â”‚  â”œâ”€ Keyword search (BM25)
    â”‚  â”œâ”€ Hybrid combination (70% + 30%)
    â”‚  â”œâ”€ Calculate confidence
    â”‚  â””â”€ Track source attribution
    â”‚
    â”œâ”€ ğŸ¤– GENERATE ANSWER
    â”‚  â”œâ”€ Build prompt with context
    â”‚  â”œâ”€ Include conversation history
    â”‚  â”œâ”€ Call LLM (with temperature control)
    â”‚  â””â”€ Return answer + confidence
    â”‚
    â”œâ”€ ğŸ“Š EVALUATE QUALITY (RAGAS)
    â”‚  â”œâ”€ Context relevance (are docs relevant?)
    â”‚  â”œâ”€ Answer relevance (does it answer question?)
    â”‚  â”œâ”€ Faithfulness (grounded, no hallucination?)
    â”‚  â”œâ”€ Compute RAG score (weighted average)
    â”‚  â””â”€ Detect hallucinations
    â”‚
    â”œâ”€ ğŸ’¾ STORE IN MEMORY
    â”‚  â”œâ”€ Save to conversation history
    â”‚  â”œâ”€ Save evaluation metrics
    â”‚  â””â”€ Persist to JSON file
    â”‚
    â”œâ”€ ğŸ”„ OPTIONAL: ADVANCED FEATURES
    â”‚  â”œâ”€ Query expansion (multiple phrasings)
    â”‚  â”œâ”€ Multi-hop reasoning (break into steps)
    â”‚  â””â”€ Adversarial testing (find weaknesses)
    â”‚
    â”œâ”€ ğŸ“ LOGGING & OBSERVABILITY
    â”‚  â”œâ”€ Log each step with emojis
    â”‚  â”œâ”€ Track metrics
    â”‚  â””â”€ Enable debugging
    â”‚
    â†“
USER OUTPUT (Answer + Sources + Confidence + Metrics)
```

## Key Architectural Decisions

### 1. Why Separate Classes?

```python
class AdaptiveChunker:
    """Only handles chunking"""

class HybridSearchEngine:
    """Only handles search"""

class RAGEvaluator:
    """Only handles evaluation"""

class QueryExpander:
    """Only handles query expansion"""

class RAGSystem:
    """Orchestrates all of the above"""
```

**Benefits:**
- Single Responsibility Principle (SRP)
- Each class does ONE thing well
- Easy to test: Test chunking separately from search
- Easy to replace: Swap HybridSearchEngine for different search
- Modular: Can use AdaptiveChunker in other projects

### 2. Why Dataclasses?

```python
@dataclass
class RAGResponse:
    answer: str
    sources: List[RetrievedDocument]
    confidence_score: float
    source_types: List[str]
    conversation_context: str
```

**vs. Dictionary Approach:**

```python
response = {
    "answer": "...",
    "sources": [...],
    "confidence": 0.87,
    ...
}
```

**Dataclasses Win:**
- Type hints (autocomplete, type checking)
- Self-documenting code
- Automatic `.to_dict()` for JSON serialization
- Enforces structure

### 3. Why Persistent Storage?

```python
# Save to files instead of only memory
CONVERSATION_HISTORY_FILE = "./conversation_history.json"
EVALUATION_METRICS_FILE = "./evaluation_metrics.json"
ADVERSARIAL_TEST_FILE = "./adversarial_test_results.json"
```

**Why?**
- Survive program restart
- Analyze patterns over time
- Audit trail (forensics)
- Share results with team

---

# PART 8: PRACTICAL EXERCISES

> **What you'll learn:** Nothing new in theory â€” this part is pure practice. Each exercise combines concepts from multiple parts, forcing you to connect theory to running code.
>
> **Prerequisites:** All of Parts 1â€“6 (ideally). At minimum: Parts 2â€“4 for exercises 1â€“3.
>
> **Time:** 2â€“3 hours spread across multiple sessions.
>
> **Tip:** Do exercises 1 and 3 first â€” embedding visualisation and weight tuning give the most intuition per hour. Return to exercises 2, 4, 5 after completing the full guide.

## Exercise 1: Understanding Embeddings

### Objective
Understand how embeddings capture meaning.

### Instructions

1. **Create visualization:**
```python
import numpy as np
import matplotlib.pyplot as plt
from sklearn.decomposition import PCA

# Get embeddings for words
words = ["Ronaldo", "Messi", "player", "car", "football", "soccer"]
embeddings = [...]  # Get from ChromaDB

# Reduce to 2D for visualization
pca = PCA(n_components=2)
reduced = pca.fit_transform(np.array(embeddings))

# Plot
plt.scatter(reduced[:, 0], reduced[:, 1])
for i, word in enumerate(words):
    plt.annotate(word, (reduced[i, 0], reduced[i, 1]))
plt.show()

# Observation: Similar words close together!
```

2. **Calculate similarities:**
```python
from sklearn.metrics.pairwise import cosine_similarity

ronaldo_emb = get_embedding("Ronaldo")
messi_emb = get_embedding("Messi")
car_emb = get_embedding("car")

sim_ronaldo_messi = cosine_similarity([ronaldo_emb], [messi_emb])[0][0]
sim_ronaldo_car = cosine_similarity([ronaldo_emb], [car_emb])[0][0]

print(f"Ronaldo vs Messi: {sim_ronaldo_messi:.2f}")  # High (close to 1)
print(f"Ronaldo vs Car: {sim_ronaldo_car:.2f}")      # Low (close to 0)
```

---

## Exercise 2: Building Your Own Evaluation

### Objective
Create custom evaluation metric beyond RAGAS.

### Instructions

```python
def evaluate_answer_conciseness(answer: str) -> float:
    """Penalize if answer is too long for the question"""

    # Questions like "How many?" should have short answers
    words = len(answer.split())

    if words < 10:
        return 0.3  # âŒ Too short, likely incomplete
    elif words > 200:
        return 0.5  # âš ï¸ Too long, requires trimming
    else:
        return 0.9  # âœ… Good length

def evaluate_source_diversity(sources: List[RetrievedDocument]) -> float:
    """Check if sources are diverse or all from one place"""

    unique_sources = set([s.source for s in sources])
    unique_types = set([s.source_type for s in sources])

    if len(unique_sources) == 1:
        return 0.5  # âš ï¸ All from same source
    elif len(unique_types) > 1:
        return 0.9  # âœ… Multiple source types
    else:
        return 0.7  # Okay

# Add to RAGASMetrics
custom_score = (
    ragas_score * 0.8 +
    evaluate_answer_conciseness(answer) * 0.1 +
    evaluate_source_diversity(sources) * 0.1
)
```

---

## Exercise 3: Experimenting with Weights

### Objective
Understand impact of hybrid search weights.

### Instructions

```python
# Test different weight combinations

weights_to_test = [
    (1.0, 0.0),   # 100% semantic only
    (0.9, 0.1),   # 90% semantic
    (0.7, 0.3),   # Current: 70% semantic
    (0.5, 0.5),   # Equal
    (0.3, 0.7),   # 30% semantic (keyword focus)
    (0.0, 1.0),   # 100% keyword only
]

for semantic_w, keyword_w in weights_to_test:
    # Update weights
    globals()['HYBRID_SEARCH_WEIGHT_SEMANTIC'] = semantic_w
    globals()['HYBRID_SEARCH_WEIGHT_KEYWORD'] = keyword_w

    # Test on queries
    queries = [
        "How many goals did Ronaldo score?",
        "Compare Ronaldo and Messi",
        "His career achievements"
    ]

    for query in queries:
        response, metrics = rag_system.process_query(query)
        print(f"Weights: {semantic_w:.1%} / {keyword_w:.1%}")
        print(f"Query: {query}")
        print(f"RAG Score: {metrics.rag_score:.2f}\n")

# Results: Which weight combination gives best average score?
```

---

## Exercise 4: Adversarial Challenge

### Objective
Create challenging test cases for your RAG system.

### Instructions

```python
# Design your own adversarial test

def create_tough_test_cases():
    """Create tests that might break the system"""

    return [
        # Test 1: Temporal ambiguity
        {
            "query": "What is Ronaldo's current team?",
            "context": "Ronaldo played for Al Nassr (2023-2025)",
            "challenge": "May not match 'current' if data is old"
        },

        # Test 2: Aggregation required
        {
            "query": "How many Ballon d'Or awards in total?",
            "context": "2008: 1, 2013: 1, 2014: 1, 2016: 1, 2017: 1",
            "challenge": "Must sum across multiple facts"
        },

        # Test 3: Contradiction
        {
            "query": "Where is Ronaldo from?",
            "context": "Sources say both 'Portugal' and 'Madeira Island'",
            "challenge": "Both technically correct, might confuse system"
        },

        # Test 4: Negation
        {
            "query": "Did Ronaldo win the World Cup?",
            "context": "Ronaldo never won a World Cup",
            "challenge": "Requires understanding negation"
        },
    ]

# Run your tests
test_cases = create_tough_test_cases()
for test in test_cases:
    response, metrics = rag_system.process_query(test["query"])
    print(f"Test: {test['challenge']}")
    print(f"Score: {metrics.rag_score:.2f}\n")
```

---

## Exercise 5: Query Expansion Analysis

### Objective
Understand impact of query expansion on retrieval.

### Instructions

```python
# Compare: With vs Without Query Expansion

query = "Compare Ronaldo and Messi's international careers"

print("WITHOUT QUERY EXPANSION:")
docs_without, _ = rag_system._retrieve_relevant_chunks(query)
print(f"  Retrieved documents: {len(docs_without)}")
for doc in docs_without:
    print(f"    - {doc.content[:50]}...")
print(f"  Confidence: {sum(d.relevance_score for d in docs_without) / len(docs_without):.2f}\n")

print("WITH QUERY EXPANSION:")
response_with, metrics_with = rag_system.process_query_with_expansion(query)
print(f"  Retrieved documents: {len(response_with.sources)}")
for doc in response_with.sources:
    print(f"    - {doc.content[:50]}...")
print(f"  Confidence: {response_with.confidence_score:.2f}")
print(f"  RAG Score: {metrics_with.rag_score:.2f}\n")

# Observation: More sources? Better RAG score? Higher confidence?
```

---

# PART 9: COMMON PITFALLS & SOLUTIONS

> **What you'll learn:** The most common mistakes when building RAG systems, and the diagnosis + fix for each. Read this after Parts 1â€“6 so the pitfalls are grounded in concepts you already understand.
>
> **Time:** 30 min
>
> **Tip:** Bookmark this section. Come back to it when something stops working â€” the symptom descriptions are written to match what you will actually see in the CLI output.

## Pitfall 1: Hallucination Spiral

### Problem
```
Query: "Who is Ronaldo's closest friend?"
Retrieved: (Nothing relevant found)
LLM: "Cristiano Ronaldo and Neymar are close friends"
     But no evidence in context! â† HALLUCINATION

User trusts answer, shares with friends
"Fact" spreads without verification
```

### Solution
```python
# 1. Check faithfulness
if metrics.faithfulness < 0.6:
    answer = "Based on provided sources, I cannot answer this question"

# 2. Require source citation
answer = "According to [Source: Wikipedia], ..."

# 3. Low confidence â†’ Escalate
if confidence < 0.5:
    answer = "I'm uncertain about this. Please verify: " + answer

# 4. Set context retrieval threshold
if len(retrieved_docs) < 2:
    answer = "Not enough information to reliably answer"
```

---

##Pitfall 2: Stale Data

### Problem
```
ChromaDB contains outdated Wikipedia data
System answers: "Ronaldo plays for Manchester United"
Reality: "Ronaldo plays for Al Nassr"

User makes decisions on false information
```

### Solution
```python
# 1. Timestamp metadata
metadata = {
    "source": "Wikipedia",
    "load_timestamp": datetime.now().isoformat(),  # Track when loaded
    "index": i
}

# 2. Periodically reload
def refresh_sources(collection_name, days=7):
    """Reload if older than 7 days"""
    if is_older_than(collection, days):
        db_client.delete_collection(collection_name)
        rag_system.load_source(source)  # Reload fresh

# 3. Warn user about age
if is_older_than(source_timestamp, 30):
    confidence *= 0.7  # Reduce confidence for old data
```

---

## Pitfall 3: Poor Chunking

### Problem
```
Document: "Ronaldo scored 850 goals.
           He played for 18 teams.
           His career spanned 20 years..."

Bad chunking (cut mid-sentence):
  Chunk 1: "Ronaldo scored 850 goals. He played for 18"
  Chunk 2: "teams. His career spanned 20 years..."

Query: "How many teams?"
Retrieved: Chunk 2 (low confidence, context lost)
```

### Solution
```python
# 1. Use adaptive chunking (already implemented!)
chunks = AdaptiveChunker.adaptive_chunk(text)

# 2. Verify chunks make sense
for chunk in chunks:
    sentences = chunk.split('.')
    if any(len(s) < 3 for s in sentences):
        print(f"âš ï¸ Bad chunk: {chunk[:50]}")

# 3. Experiment with chunk sizes
for chunk_size in [200, 500, 800, 1200]:
    results = evaluate_with_chunk_size(chunk_size)
    print(f"Chunk size {chunk_size}: RAG score = {results['rag_score']:.2f}")
```

---

## Pitfall 4: Ignoring Context Window

### Problem
```
LLM has context window of 4000 tokens (â‰ˆ13,000 chars)

You pass:
  - System prompt: 500 tokens
  - Retrieved context: 2000 tokens
  - Conversation history: 1500 tokens
  - User query: 50 tokens
  TOTAL = 4050 tokens â† EXCEEDS LIMIT!

Result: LLM cuts off, loses important information
```

### Solution
```python
# 1. Estimate token count
def estimate_tokens(text):
    """Rough estimate: 1 token â‰ˆ 4 characters"""
    return len(text) / 4

# 2. Limit conversation history
def _build_conversation_context(self, max_messages=4):
    # Only use last 4 messages
    recent = self.conversation_history[-max_messages:]

# 3. Prioritize context
limited_context = f"""Based on the top-3 most relevant documents:

{context[:1500]}  # Only include first 1500 chars

Question: {user_query}"""

# 4. Check before sending
total_tokens = estimate_tokens(prompt)
if total_tokens > 3800:  # Leave buffer
    raise Warning(f"Context too large: {total_tokens} tokens")
```

---

## Pitfall 5: Not Testing Edge Cases

### Problem
```
System works on normal queries
But fails on:
  - Empty input
  - Very long input
  - Special characters:
  - Non-English text

Released to production â†’ Crashes on real user input
```

### Solution
```python
# Already implemented: Adversarial testing!
def run_adversarial_tests(self):
    test_cases = AdversarialTestSuite.generate_test_cases()

    for test in test_cases:
        result = run_test_case(rag_system, test)
        if not result.passed:
            print(f"âŒ FAILURE: {test.test_id}")
            print(f"   Query: {test.query}")
            print(f"   Error: {result.error_message}")

# Add more edge cases
extra_tests = [
    ("", "Empty query"),                    # Empty
    ("a" * 5000, "Very long query"),        # Length
    ("ğŸ˜€ğŸ‰ğŸš€", "Emojis only"),             # Unicode
    ("SELECT * FROM users", "SQL injection"), # Malicious
    ("What?\n\n\n???", "Weird formatting"), # Formatting
]
```

---

## Pitfall 6: Not Versioning Experiments

### Problem
```
You modify chunk size from 500 to 800
RAG score improves 0.85 â†’ 0.91

6 months later: "What changed?"
Can't remember!
```

### Solution
```python
# 1. Log configuration
config_log = {
    "timestamp": datetime.now().isoformat(),
    "chunk_size": 1024,     # actual default for academic content
    "chunk_overlap": 200,
    "semantic_weight": 0.7,
    "keyword_weight": 0.3,
    "temperature": 0.2,
    "results": {
        "avg_rag_score": 0.91,
        "hallucination_rate": 0.05,
        "average_confidence": 0.87
    }
}

# 2. Store experiments
experiments_log.append(config_log)

# 3. Compare configurations
for exp in experiments_log:
    if exp["chunk_size"] == 1024:
        print(f"Chunk 1024: RAG = {exp['results']['avg_rag_score']}")
    if exp["chunk_size"] == 512:
        print(f"Chunk 512: RAG = {exp['results']['avg_rag_score']}")
```

---

# PART 7: ADVANCED SYSTEM FEATURES

> **What you'll learn:** Seven production-grade features that make this system production-ready â€” DomainGuard for detecting off-topic queries, Self-Query Decomposition for multi-aspect questions, LRU Embedding Cache for performance, FactChecker for claim-level verification, JSONStorage for persisting sessions, Document Reranking for improved precision, and Passage Highlighting for transparency.
>
> **Prerequisites:** All of Parts 1â€“6. These are the most advanced sections: they assume you understand embeddings, the pipeline, and evaluation.
>
> **Time:** 90-120 min
>
> **After this part:** You understand every component in the system and can explain the complete NotebookLM-style RAG pipeline. Read [ARCHITECTURE.md](./ARCHITECTURE.md) to see all the design decisions in one place.

## 7.1 Out-of-Domain Detection (DomainGuard)

### The Problem

A RAG system loaded with Wikipedia's "Machine Learning" page should refuse (or warn) when asked "What are the lyrics to Bohemian Rhapsody?"  Without a guard, the LLM generates a plausible-sounding but completely unsupported answer.

### How DomainGuard Works

`src/retrieval/domain_guard.py` â€” `DomainGuard` class

**Step 1 â€” Build a domain profile when sources are loaded:**

```python
def update_profile(self, chunks: List[str]):
    """Called after every load; builds rolling snapshot of loaded content"""
    # Sample up to 50 representative chunks
    self._sample_chunks = (self._sample_chunks + chunks)[-50:]

    # Extract key topics via LLM
    topics = self._extract_topics(self._sample_chunks)
    self._domain_topics = topics
```

**Step 2 â€” LLM topic extraction:**

```python
def _extract_topics(self, chunks: List[str]) -> List[str]:
    """Ask LLM: what topics does this content cover?"""
    prompt = f"""List the main topics covered in these text excerpts.
Return a comma-separated list of specific topics only.

Excerpts: {combined_text}"""
    # Returns: ["machine learning", "neural networks", "supervised learning", ...]
```

**Step 3 â€” Score incoming query:**

```python
def _llm_relevance_score(self, query: str) -> float:
    """LLM rates 0â€“10 how relevant the query is to the domain topics"""
    prompt = f"""Rate 0â€“10 how relevant this question is to these topics.
Topics: {self._domain_topics}
Question: {query}
Return only a number."""
    # Normalise: score / 10 â†’ 0.0â€“1.0
```

**Step 4 â€” Compare against threshold:**

```python
DOMAIN_SIMILARITY_THRESHOLD = 0.35   # from config

result = domain_guard.check(query)
if result.similarity_score < 0.35:
    print(f"âš ï¸  Query may be outside loaded domain (score={result.similarity_score:.2f})")
    # Warning only â€” system still proceeds; user makes the call
```

### Key Design: Warning Not Block

```
WHY warn instead of block?
  - False positives are costly: a legitimate question mis-classified as out-of-domain
    would silently fail the user.
  - The user knows their intent better than the guard.
  - DomainGuard is a sanity check, not a gatekeeper.
  - threshold=0.35 is deliberately low to minimise false positives.
```

### Exercise

Load two different Wikipedia pages. Run `domain-stats` to see the combined topic profile. Then ask an obviously unrelated question and observe the warning score.

---

## 7.2 Self-Query Decomposition

### The Problem

```
Query: "What is machine learning, what are its main types, and how is it used in healthcare?"

Single retrieval pass:
  Search returns docs about "machine learning definition"
  Types and healthcare applications get no coverage
  â†’ Incomplete answer
```

### Self-Query vs Multi-Hop

Both break complex queries into smaller pieces, but they differ fundamentally:

```
SELF-QUERY DECOMPOSITION (SelfQueryDecomposer)
  Purpose: Cover multiple INDEPENDENT aspects of one question
  Execution: PARALLEL (each sub-query retrieves independently)
  Example: "What is X, how does Y work, and where is Z used?"
    â†’ Sub-query 1: "What is X?"           â† no dependency on 2 or 3
    â†’ Sub-query 2: "How does Y work?"     â† no dependency on 1 or 3
    â†’ Sub-query 3: "Where is Z used?"     â† no dependency on 1 or 2
  All sub-queries retrieve simultaneously, results merged, ONE LLM call

MULTI-HOP REASONING (MultiHopReasoner)
  Purpose: Answer a question that REQUIRES prior steps
  Execution: SEQUENTIAL (each step feeds the next)
  Example: "How did Einstein's work lead to nuclear energy?"
    â†’ Step 1: "What is E=mcÂ²?"            â† needed to understand step 2
    â†’ Step 2: "How does mass-energy equivalence relate to fission?"  â† needed for step 3
    â†’ Step 3: "How was fission applied to generate energy?"
  Each step's answer informs the next search; separate LLM synthesis
```

### Implementation

```python
# src/reasoning/self_query_decomposer.py

@dataclass
class SubQuery:
    text: str          # The focused sub-question text
    aspect: str        # Human-readable label ("definition", "types", "applications")
    index: int         # Position in decomposition order

class SelfQueryDecomposer:

    def analyze_complexity(self, query: str) -> bool:
        """Return True if query contains multiple independent aspects"""
        # Signals: conjunctions ("and", "also"), aspect keywords ("what", "how", "where"),
        # multiple question marks, long queries
        ...

    def decompose(self, query: str) -> List[SubQuery]:
        """Break multi-aspect query into focused sub-queries"""
        prompt = f"""Break this question into independent sub-questions.
Each sub-question should address one specific aspect.
Return as JSON list: [{{"text": "...", "aspect": "..."}}]

Question: {query}"""
        ...

    def synthesize(self, original_query: str, sub_results: List[Dict]) -> str:
        """Merge sub-query answers into one coherent response"""
        # Single LLM call with all sub-query results as context
        ...
```

### Flow

```
Complex query
      â†“
analyze_complexity() â†’ True?
      â†“  Yes
decompose() â†’ [SubQuery1, SubQuery2, SubQuery3]
      â†“
PARALLEL:
  SubQuery1 â†’ Hybrid Search â†’ Docs1
  SubQuery2 â†’ Hybrid Search â†’ Docs2
  SubQuery3 â†’ Hybrid Search â†’ Docs3
      â†“
Merge & deduplicate all docs
      â†“
synthesize(original_query, merged_docs) â†’ Final Answer
```

---

## 7.3 LRU Embedding Cache

### The Problem

Embedding computation is the most common bottleneck in a RAG query:

```
Cold query flow:
  Query text â†’ ChromaDB embed() â†’ [0.12, 0.87, ...] (1536-dim vector)
  Time: 100â€“200 ms per call

Repeated or similar queries:
  Same text â†’ same embedding â†’ why recompute?
```

### LRU Cache Design

`src/retrieval/cache.py` â€” `EmbeddingCache` class

```python
from collections import OrderedDict
import hashlib

class EmbeddingCache:
    def __init__(self, max_size: int = 1000):
        self._cache: OrderedDict = OrderedDict()
        self._max_size = max_size
        self._hits = 0
        self._misses = 0

    def _key(self, text: str) -> str:
        """MD5 hash of the query text â†’ compact, collision-safe key"""
        return hashlib.md5(text.encode()).hexdigest()

    def get(self, text: str):
        key = self._key(text)
        if key in self._cache:
            # LRU: move to end (most-recently-used position)
            self._cache.move_to_end(key)
            self._hits += 1
            return self._cache[key]
        self._misses += 1
        return None

    def set(self, text: str, embedding):
        key = self._key(text)
        self._cache[key] = embedding
        self._cache.move_to_end(key)
        # Evict least-recently-used (first item) when at capacity
        if len(self._cache) > self._max_size:
            self._cache.popitem(last=False)  # â† FIFO eviction from head = LRU eviction
```

### Why OrderedDict?

```
dict (Python 3.7+) maintains insertion order but has no move_to_end()
OrderedDict adds:
  - move_to_end(key)     â†’ promote a key to most-recently-used
  - popitem(last=False)  â†’ remove least-recently-used (head of dict)

This gives O(1) get, set, and eviction â€” perfect for an LRU cache.
```

### Performance Impact

```
Cache cold (first query):  100â€“200 ms  (ChromaDB embeds)
Cache warm (repeat query): <1 ms       (~100-200x speedup)

Typical session:
  First load: miss
  Follow-up questions on same topic: ~50% hit rate
  Repeated context-window queries: 90%+ hit rate

View stats: run 'cache' command in CLI
```

---

## 7.4 Fact Checker

### Purpose

RAGAS Faithfulness measures aggregate support. The FactChecker goes further: it assigns a **verdict** to each individual claim, telling you exactly which facts are supported, which are contradicted, and which cannot be verified.

### Three-State Verdict System

`src/evaluation/fact_checker.py` â€” `FactChecker` class

```
SUPPORTED     â€” Context explicitly confirms this claim
CONTRADICTED  â€” Context says something that directly conflicts with this claim
UNKNOWN       â€” Context doesn't mention this claim either way
```

```python
# Example output for: "Ronaldo scored 450 Real Madrid goals and won 10 Ballon d'Ors"

Claim 1: "Ronaldo scored 450 Real Madrid goals"
  Verdict: SUPPORTED    confidence=0.92
  Evidence: "...scored 450 goals in 438 appearances for Real Madrid..."

Claim 2: "won 10 Ballon d'Ors"
  Verdict: CONTRADICTED confidence=0.88
  Evidence: "...Ronaldo has won 5 Ballon d'Or awards..."

Claim 3: (implicit) "played for Real Madrid"
  Verdict: SUPPORTED    confidence=0.99
```

### Implementation Details

```python
class FactChecker:

    MAX_FACTS = 5     # Cap claims per answer â€” avoids token explosion
    MIN_CLAIM_LEN = 10  # Skip trivially short strings (e.g., "Yes", "In 2023")

    def check_answer(self, answer: str, context_docs: List[str]) -> List[FactCheckResult]:
        claims = self._extract_claims(answer)   # LLM parses answer into atomic claims
        results = []
        for claim in claims[:self.MAX_FACTS]:
            if len(claim) < self.MIN_CLAIM_LEN:
                continue
            verdict, confidence, evidence = self._verify_claim(claim, context_docs)
            results.append(FactCheckResult(
                claim=claim,
                verdict=verdict,         # "SUPPORTED" | "CONTRADICTED" | "UNKNOWN"
                confidence=confidence / 100,  # LLM returns 0â€“100, normalise to 0â€“1
                supporting_passages=evidence,
            ))
        return results
```

### Confidence Normalisation

```
LLM returns: 85  (out of 100, natural for humans to think in percentages)
System stores: 0.85  (0â€“1 is standard for ML confidence scores)

Formula: confidence = llm_score / 100
```

### When to Use

```
Fact-check is off by default (toggle: 'fact-check' command)
Use when:
  - Answering factual queries where errors are high-cost
  - Debugging why an answer seems wrong
  - Validating against a high-quality source you just loaded

Cost: 300â€“500 ms per answer (1 extra LLM call for claim extraction + 1 per claim)
```

---

## 7.5 JSONStorage Persistence

### Why Persistence Matters

```
Without persistence:
  Session 1: Ask 20 questions, get RAGAS scores
  Session ends: All data lost
  Session 2: No history, no metrics, no way to compare

With persistence:
  Every query + answer + RAGAS scores â†’ saved to disk
  Analyse quality trends over time
  Resume conversations
```

### Design: Abstract Storage Interface

`src/persistence/__init__.py`

```python
from abc import ABC, abstractmethod

class Storage(ABC):
    """Abstract base â€” swap backends without changing callers"""

    @abstractmethod
    def save(self, key: str, data: dict) -> None: ...

    @abstractmethod
    def load(self, key: str) -> dict: ...

    @abstractmethod
    def list_keys(self) -> List[str]: ...

    @abstractmethod
    def export(self, filepath: str) -> None: ...


class JSONStorage(Storage):
    """Concrete implementation: plain JSON files"""

    def __init__(self, data_dir: str = "./json_data"):
        self.data_dir = data_dir
        os.makedirs(data_dir, exist_ok=True)   # â† Auto-creates directory if absent
```

### What Gets Saved

```
json_data/
  conversation_history.json    â† Every query + answer turn
  evaluation_metrics.json      â† RAGAS scores per query

Format (conversation_history.json):
[
  {
    "role": "user",
    "content": "What is supervised learning?",
    "timestamp": "2026-02-27T10:15:42"
  },
  {
    "role": "assistant",
    "content": "Supervised learning is...",
    "sources": ["Machine Learning (Wikipedia)"],
    "ragas_score": 0.87,
    "timestamp": "2026-02-27T10:15:44"
  }
]
```

### Save Trigger

```python
# JSONStorage.save() is called automatically after every query.
# Manual save: 'save' command (or 'save my_session.json')
# If save fails: error is logged, CLI continues (no crash)
```

### Design Pattern: Strategy via Inheritance

```
Storage (ABC)
    â”‚
    â””â”€â”€ JSONStorage     â† current implementation
    â””â”€â”€ [Future] SQLiteStorage   â† swap in without changing RAGSystem
    â””â”€â”€ [Future] CloudStorage    â† same interface, different backend

RAGSystem holds a Storage reference, never a JSONStorage reference:
  self.storage: Storage = JSONStorage(config.data_dir)
```

---

## 7.6 Document Reranking (Cross-Encoder + MMR)

### The Problem: Initial Retrieval is Fast but Imprecise

```
Hybrid Search (semantic + BM25):
  âœ… Fast: 50-150ms to retrieve 50 candidates
  âŒ Imprecise: Uses bi-encoder embeddings (query and docs encoded separately)
  âŒ No diversity: May return near-duplicate documents

Result: Top 5 documents may not be the BEST 5 â€” just the FASTEST 5 to find.
```

**Two-Stage Retrieval** solves this:

```
STAGE 1: Bi-encoder (semantic + BM25 hybrid)
  â†’ Retrieve 50 candidates          (fast, broad coverage)

STAGE 2: Reranking
  Step 2a: Cross-encoder scoring    (accurate relevance)
  Step 2b: MMR diversity filter     (remove redundancy)
  â†’ Select top 5 documents           (high quality, diverse)
```

### Cross-Encoder vs Bi-Encoder

**Bi-encoder (used in initial retrieval):**

```python
# Stage 1: Encode query and documents SEPARATELY
query_embedding = embed(query)              # [768 dimensions]
doc_embeddings = [embed(doc) for doc in docs]  # List of [768 dims]

# Similarity: simple cosine/dot product
similarity = cosine(query_embedding, doc_embedding)
```

**Advantages:**
- âœ… Fast: Documents can be pre-embedded and cached
- âœ… Scalable: Compare 1 query embedding vs millions of doc embeddings
- âœ… Storage efficient: Store embeddings once

**Limitations:**
- âŒ Query and document never "see" each other during encoding
- âŒ Misses cross-attention: Can't model query-document interactions
- âŒ Lower precision: ~65-75% top-5 accuracy

**Cross-encoder (used in reranking):**

```python
# Stage 2: Encode query + document JOINTLY
for doc in candidates:
    # Concatenate and encode together
    input_text = f"[CLS] {query} [SEP] {doc} [SEP]"
    score = cross_encoder.predict(input_text)  # Single relevance score
```

**Advantages:**
- âœ… Accurate: Query and document attend to each other (transformer cross-attention)
- âœ… Higher precision: ~85-95% top-5 accuracy
- âœ… Captures semantic nuances that bi-encoders miss

**Limitations:**
- âŒ Slow: Must encode each (query, doc) pair individually â€” 50 pairs = 50 forward passes
- âŒ Not cacheable: Each query needs fresh computation
- âŒ Cannot pre-compute: Must wait for query to arrive

### Why Two-Stage Works

```
Analogy: Hiring Pipeline

STAGE 1 (Bi-encoder): Resume screening
  - Fast filter: 1000 applicants â†’ 50 candidates (based on keywords, education)
  - Goal: Recall (don't miss good candidates)

STAGE 2 (Cross-encoder): In-depth interviews
  - Slow evaluation: 50 candidates â†’ 5 finalists (holistic assessment)
  - Goal: Precision (only select the BEST)
```

### Cross-Encoder Model Architecture

**Model used:** `cross-encoder/ms-marco-MiniLM-L-12-v2`

```
Training:
  - Trained on MS MARCO dataset (Microsoft Machine Reading Comprehension)
  - 8.8M query-document pairs with relevance labels
  - Fine-tuned BERT-style transformer (12 layers, 384 hidden dim)

Input format:
  [CLS] query tokens [SEP] document tokens [SEP]

Output:
  Single relevance score (typically -10 to +10)
  Higher score = more relevant

Size: 84MB (small enough for CPU inference)
Latency: ~5-10ms per pair (on GPU), ~30-50ms per pair (on CPU)
```

### Implementation

`src/retrieval/reranker.py`

```python
from sentence_transformers import CrossEncoder

class DocumentReranker:

    def __init__(self, model_name="cross-encoder/ms-marco-MiniLM-L-12-v2"):
        self.cross_encoder = CrossEncoder(model_name)

    def rerank(
        self,
        query: str,
        documents: List[RetrievedDocument],
        top_k: int = 5
    ) -> List[RetrievedDocument]:
        """Rerank documents using cross-encoder + MMR"""

        # Step 1: Cross-encoder scoring
        pairs = [[query, doc.content] for doc in documents]
        scores = self.cross_encoder.predict(pairs)  # Returns array of scores

        # Update document distances (convert score to distance)
        for i, doc in enumerate(documents):
            # Normalize score: cross-encoder outputs ~[-10, 10]
            # Convert to distance: higher score â†’ lower distance
            doc.distance = 1.0 - (scores[i] / 10.0)
            doc.distance = max(0.0, min(2.0, doc.distance))  # Clamp to [0, 2]

        # Sort by distance (lower is better)
        documents.sort(key=lambda d: d.distance)

        # Step 2: MMR diversity filter
        if self.config.use_mmr:
            documents = self._apply_mmr(query, documents, top_k)

        return documents[:top_k]
```

### Performance Impact

```
Without reranking:
  Query latency: 150ms
  Top-5 precision: ~70%

With reranking (50 candidates â†’ top 5):
  Query latency: 350ms (+200ms for cross-encoder)
  Top-5 precision: ~88% (+18 percentage points)

Breakdown:
  - Bi-encoder retrieval: 150ms
  - Cross-encoder inference: 50 docs Ã— 4ms = 200ms
  - MMR computation: negligible
```

**When to enable:**
- High-stakes queries where accuracy matters more than speed
- Research applications (citation finding)
- Customer-facing QA where wrong answers are costly

**When to disable:**
- Real-time chat (latency-sensitive)
- High query volume (compute-intensive)
- Good-enough accuracy from hybrid search alone

---

### MMR: Maximal Marginal Relevance

### The Diversity Problem

```
Query: "What are the types of machine learning?"

Without MMR â€” Top 5 docs after cross-encoder:
  1. "Machine learning has three main types: supervised..."         â† Relevant
  2. "The primary categories of ML are supervised, unsupervised..." â† REDUNDANT with #1
  3. "ML types include supervised, unsupervised..."                â† REDUNDANT with #1 & #2
  4. "Supervised learning is when..."                              â† Focuses on one type
  5. "Deep learning is a subset..."                                â† Tangential

Problem: Top 3 documents say nearly the same thing!
  â†’ Wastes 2 of 5 slots on duplicate information
  â†’ User doesn't get comprehensive coverage
```

### MMR Algorithm

**Goal:** Balance relevance and diversity
- **Relevance:** Document answers the query well
- **Diversity:** Document adds NEW information not in already-selected docs

**Formula:**

```
MMR(d) = Î» Ã— Relevance(query, d) - (1-Î») Ã— max Similarity(d, selected_docs)

Where:
  Î» = 0.0 â†’ maximize diversity (ignore relevance)
  Î» = 1.0 â†’ maximize relevance (ignore diversity)
  Î» = 0.7 â†’ balanced (default)
```

**Intuition:**
- High relevance â†’ increase MMR score
- High similarity to already-selected docs â†’ decrease MMR score
- Select documents that are relevant BUT different from what we've already chosen

### Step-by-Step Example

```
Query: "What are the types of machine learning?"
Î» = 0.7 (relevance weight)

Candidates after cross-encoder:
  Doc A: relevance=0.95, content="ML has three types: supervised, unsupervised, reinforcement"
  Doc B: relevance=0.92, content="The main ML categories are supervised, unsupervised..."
  Doc C: relevance=0.85, content="Supervised learning uses labeled data..."
  Doc D: relevance=0.80, content="Reinforcement learning involves agents and rewards..."
  Doc E: relevance=0.78, content="Applications of ML in healthcare..."

STEP 1: Select highest relevance document
  Selected: [Doc A]  (relevance=0.95)

STEP 2: Compute MMR for remaining candidates
  Doc B:
    Relevance component: 0.7 Ã— 0.92 = 0.644
    Diversity penalty: 0.3 Ã— similarity(B, A) = 0.3 Ã— 0.88 = 0.264  â† very similar to A!
    MMR(B) = 0.644 - 0.264 = 0.380

  Doc C:
    Relevance: 0.7 Ã— 0.85 = 0.595
    Diversity: 0.3 Ã— similarity(C, A) = 0.3 Ã— 0.45 = 0.135  â† different from A (deep dive on one type)
    MMR(C) = 0.595 - 0.135 = 0.460  â† WINNER for slot #2

  Doc D:
    Relevance: 0.7 Ã— 0.80 = 0.560
    Diversity: 0.3 Ã— similarity(D, A) = 0.3 Ã— 0.50 = 0.150
    MMR(D) = 0.560 - 0.150 = 0.410

  Doc E:
    Relevance: 0.7 Ã— 0.78 = 0.546
    Diversity: 0.3 Ã— similarity(E, A) = 0.3 Ã— 0.20 = 0.060  â† very different!
    MMR(E) = 0.546 - 0.060 = 0.486

  Selected: [Doc A, Doc C]  (C had highest MMR)

STEP 3: Repeat for remaining slots
  Now compute MMR against BOTH A and C
  Select doc with highest score
  Continue until k=5 documents selected

Final selection: [Doc A, Doc C, Doc E, Doc D, ...]
  â†’ Doc B skipped despite high relevance (too similar to A)
  â†’ Doc E included despite lower relevance (adds new perspective: applications)
```

### Implementation

```python
def _apply_mmr(
    self,
    query: str,
    documents: List[RetrievedDocument],
    top_k: int,
    lambda_param: float = 0.7
) -> List[RetrievedDocument]:
    """Apply MMR for diversity"""

    if len(documents) <= top_k:
        return documents

    # Get relevance scores (already computed by cross-encoder)
    relevance = [doc.relevance_score for doc in documents]

    # Initialize
    selected_indices = []
    remaining_indices = list(range(len(documents)))

    # Select first document (highest relevance)
    first_idx = max(remaining_indices, key=lambda i: relevance[i])
    selected_indices.append(first_idx)
    remaining_indices.remove(first_idx)

    # Incrementally select documents
    while len(selected_indices) < top_k and remaining_indices:
        mmr_scores = []

        for idx in remaining_indices:
            # Relevance component
            rel = lambda_param * relevance[idx]

            # Diversity component (max similarity to any selected doc)
            max_sim = max([
                self._compute_similarity(
                    documents[idx].content,
                    documents[sel_idx].content
                )
                for sel_idx in selected_indices
            ])

            # MMR formula
            mmr = rel - ((1 - lambda_param) * max_sim)
            mmr_scores.append(mmr)

        # Select document with highest MMR
        best_mmr_idx = max(range(len(mmr_scores)), key=lambda i: mmr_scores[i])
        best_doc_idx = remaining_indices[best_mmr_idx]
        selected_indices.append(best_doc_idx)
        remaining_indices.remove(best_doc_idx)

    return [documents[i] for i in selected_indices]

def _compute_similarity(self, text1: str, text2: str) -> float:
    """Compute Jaccard similarity for diversity estimation"""
    words1 = set(text1.lower().split())
    words2 = set(text2.lower().split())

    intersection = words1 & words2
    union = words1 | words2

    return len(intersection) / len(union) if union else 0.0
```

### Tuning Î» (Lambda)

```
Î» = 0.0 (max diversity):
  Result: Documents cover many different topics
  Risk: Some documents may be barely relevant
  Use case: Exploratory search, brainstorming

Î» = 0.5 (balanced):
  Result: Equal weight to relevance and diversity
  Use case: General-purpose retrieval

Î» = 0.7 (default, relevance-focused):
  Result: Prioritize relevance, but avoid exact duplicates
  Use case: Question answering (this system)

Î» = 1.0 (max relevance):
  Result: Ignore diversity entirely (same as no MMR)
  Risk: May retrieve near-duplicate documents
  Use case: When you want ALL mentions of a specific fact
```

**Configuration:**

```python
# src/config.py

class SearchConfig:
    enable_reranking: bool = False
    use_cross_encoder: bool = True
    use_mmr_diversity: bool = True
    mmr_lambda: float = 0.7  # Balance relevance vs diversity
```

### CLI Usage

```bash
> rerank
ğŸ”„ Reranking (cross-encoder + MMR): âœ… ENABLED
   Cross-encoder: True
   MMR diversity: True (Î»=0.7)

> query What are the main types of machine learning?

ğŸ’¡ ANSWER:
================================================================================
Machine learning has three primary types: supervised learning, unsupervised
learning, and reinforcement learning. Supervised learning uses labeled data...
================================================================================
ğŸ“Š Confidence: 92%
ğŸ”„ Reranking: Applied (cross-encoder + MMR diversity)
```

### Exercise: Compare With and Without Reranking

```bash
# Test 1: Without reranking
> rerank  # Disable
> load wikipedia "Machine Learning"
> query What is the difference between supervised and unsupervised learning?
> metrics
# Note the RAGAS scores

# Test 2: With reranking
> rerank  # Enable
> query What is the difference between supervised and unsupervised learning?
> metrics
# Compare: Did answer quality improve? Did latency increase?
```

**Expected observations:**
- Faithfulness score increases (better grounding)
- Context relevance improves (better documents selected)
- Latency increases by 150-250ms

---

## 7.7 Passage Highlighting & Source Attribution

### The Transparency Problem

```
Without passage highlighting:

User: "What is supervised learning?"

Assistant: "Supervised learning is a machine learning approach where models
learn from labeled training data to make predictions on new data."

User: "Where did you get that?"

Assistant: "Source: wikipedia Machine Learning (3,500 words)"

User: "Can you show me the exact part?"

Assistant: ğŸ¤· (no way to pinpoint the specific sentences)
```

**Problem:** User must manually search through 3,500 words to verify the claim.

**Solution: Passage highlighting**

```
With passage highlighting:

ğŸ“ HIGHLIGHTED PASSAGES (Top Relevant Excerpts)
================================================================================

[Passage 1] Relevance: 92%
Source: wikipedia Machine Learning
ğŸ“ "Supervised learning is a machine learning paradigm where algorithms learn
     from labeled training data, finding patterns to make predictions on new,
     unseen data."

[Passage 2] Relevance: 84%
Source: wikipedia Machine Learning
ğŸ“ "In supervised learning, each training example consists of an input object
     and a desired output value, allowing the algorithm to learn the mapping."
```

**Benefits:**
1. âœ… **Verification:** User can immediately check if answer is grounded
2. âœ… **Trust:** Transparency builds confidence in the system
3. âœ… **Debugging:** Developers see exactly what was retrieved
4. âœ… **Citations:** Ready-made quotes for reports/papers

### How It Works

**Pipeline:**

```
Query â†’ Retrieve Docs â†’ Generate Answer â†’ Extract Passages â†’ Display
         (5 docs)                          (15 passages)    (top 5)
```

**Step-by-step:**

1. **Document retrieval** (existing): Get top 5 documents via hybrid search + reranking
2. **Answer generation** (existing): LLM generates answer from documents
3. **Passage extraction** (NEW): Split documents into sentences
4. **Relevance scoring** (NEW): Score each sentence against query + answer
5. **Ranking** (NEW): Sort passages by relevance
6. **Display** (NEW): Show top N passages with source attribution

### Sentence Segmentation

**Challenge:** How do you split a document into passages?

```
Option 1: Fixed-length chunks (e.g., 100 characters)
  âŒ Splits mid-sentence, hard to read

Option 2: Paragraph boundaries
  âŒ Paragraphs vary wildly (1 sentence to 20 sentences)

Option 3: Sentence boundaries (CHOSEN)
  âœ… Natural reading units
  âœ… Complete thoughts
  âœ… NLTK Punkt tokenizer handles abbreviations (Dr., U.S., etc.)
```

**Implementation:**

```python
import nltk
from nltk.tokenize import sent_tokenize

# Download NLTK data (once)
nltk.download('punkt')

text = """
Machine learning (ML) is a field of AI. It uses statistical algorithms.
Dr. Smith founded ML Labs in 2020. The U.S. leads in ML research.
"""

sentences = sent_tokenize(text)
# Result:
# ['Machine learning (ML) is a field of AI.',
#  'It uses statistical algorithms.',
#  'Dr. Smith founded ML Labs in 2020.',      â† Punkt handles "Dr."
#  'The U.S. leads in ML research.']         â† Punkt handles "U.S."
```

### Relevance Scoring Algorithm

**Goal:** Score each sentence based on how relevant it is to the query and answer.

**Approach: Keyword-based scoring** (simple but effective)

```python
def calculate_relevance(
    sentence: str,
    query_keywords: Set[str],
    answer_keywords: Set[str],
    doc_relevance: float
) -> float:
    """
    Score = (query_overlap Ã— 0.6) + (answer_overlap Ã— 0.4) + (doc_score Ã— 0.3)
    """
    sentence_words = set(sentence.lower().split())

    # Query overlap: What % of query keywords appear in sentence?
    query_matches = len(query_keywords & sentence_words)
    query_ratio = query_matches / len(query_keywords) if query_keywords else 0

    # Answer overlap: What % of answer keywords appear in sentence?
    answer_matches = len(answer_keywords & sentence_words)
    answer_ratio = answer_matches / len(answer_keywords) if answer_keywords else 0

    # Document relevance: Boost sentences from high-scoring documents
    doc_boost = doc_relevance * 0.3

    # Weighted combination
    score = (query_ratio * 0.6) + (answer_ratio * 0.4) + doc_boost

    # Bonus: Exact phrase match
    for keyword in query_keywords:
        if len(keyword) > 4 and keyword in sentence.lower():
            score += 0.1

    return min(score, 1.0)  # Cap at 1.0
```

**Why this works:**

- **Query keywords** (60% weight): Sentence should be relevant to what user asked
- **Answer keywords** (40% weight): Sentence should support what was answered
- **Document score** (30% boost): Trust sentences from highly-ranked docs
- **Phrase bonus**: Reward exact multi-word matches ("machine learning" > "machine" + "learning")

### Example Scoring

```
Query: "What is supervised learning?"
Answer: "Supervised learning is when algorithms learn from labeled data."

Query keywords: {supervised, learning}
Answer keywords: {supervised, learning, algorithms, labeled, data}

Sentence 1: "Supervised learning uses labeled training data."
  Query overlap: 2/2 = 100% â†’ 0.6 Ã— 1.0 = 0.60
  Answer overlap: 3/5 = 60%  â†’ 0.4 Ã— 0.6 = 0.24
  Doc score boost: 0.3 Ã— 0.85 = 0.255
  Phrase bonus: "supervised learning" found â†’ +0.1
  Total: 0.60 + 0.24 + 0.255 + 0.1 = 1.195 â†’ capped to 1.0
  âœ… HIGHLY RELEVANT

Sentence 2: "Machine learning has many applications in healthcare."
  Query overlap: 1/2 = 50%   â†’ 0.6 Ã— 0.5 = 0.30
  Answer overlap: 0/5 = 0%   â†’ 0.4 Ã— 0.0 = 0.00
  Doc score boost: 0.3 Ã— 0.85 = 0.255
  Phrase bonus: 0
  Total: 0.30 + 0.00 + 0.255 = 0.555
  âš ï¸ MODERATELY RELEVANT (mentions "learning" but not "supervised")

Sentence 3: "Deep learning uses neural networks with many layers."
  Query overlap: 1/2 = 50%   â†’ 0.6 Ã— 0.5 = 0.30
  Answer overlap: 0/5 = 0%   â†’ 0.4 Ã— 0.0 = 0.00
  Doc score boost: 0.3 Ã— 0.85 = 0.255
  Phrase bonus: 0
  Total: 0.30 + 0.00 + 0.255 = 0.555
  âš ï¸ MODERATELY RELEVANT (tangential topic)
```

### Implementation

`src/retrieval/passage_highlighter.py`

```python
from dataclasses import dataclass
from nltk.tokenize import sent_tokenize

@dataclass
class HighlightedPassage:
    text: str                  # The sentence text
    document_source: str       # Which document it came from
    relevance_score: float     # 0-1 relevance score
    sentence_index: int        # Position in original document

class PassageHighlighter:

    def __init__(self, max_passages_per_doc: int = 3):
        self.max_passages_per_doc = max_passages_per_doc

    def extract_relevant_passages(
        self,
        query: str,
        documents: List[RetrievedDocument],
        answer: str = None
    ) -> List[HighlightedPassage]:
        """Extract top relevant passages from all documents"""

        # Extract keywords
        query_keywords = self._extract_keywords(query)
        answer_keywords = self._extract_keywords(answer) if answer else set()

        all_passages = []

        # Process each document
        for doc in documents:
            sentences = sent_tokenize(doc.content)

            for idx, sentence in enumerate(sentences):
                if len(sentence.strip()) < 10:  # Skip trivial sentences
                    continue

                # Score this sentence
                score = self._calculate_relevance(
                    sentence,
                    query_keywords,
                    answer_keywords,
                    doc.relevance_score
                )

                passage = HighlightedPassage(
                    text=sentence.strip(),
                    document_source=doc.source,
                    relevance_score=score,
                    sentence_index=idx
                )
                all_passages.append(passage)

        # Sort by relevance and return top passages
        all_passages.sort(key=lambda p: p.relevance_score, reverse=True)

        # Limit: max_passages_per_doc * num_documents
        max_total = len(documents) * self.max_passages_per_doc
        return all_passages[:max_total]

    def _extract_keywords(self, text: str) -> Set[str]:
        """Extract meaningful keywords (filter stopwords)"""
        stopwords = {'the', 'a', 'an', 'and', 'or', 'but', 'in', 'on', 'at', ...}
        words = text.lower().split()
        return {w for w in words if w not in stopwords and len(w) > 2}
```

### CLI Integration

```bash
# Enable passage highlighting
> highlight
ğŸ“ Passage highlighting: âœ… ENABLED

# Run a query
> query What is supervised learning?

ğŸ’¡ ANSWER:
================================================================================
Supervised learning is a machine learning approach...
================================================================================
ğŸ“Š Confidence: 88%
ğŸ“ Highlighted Passages: 5 relevant excerpts found
   Run 'passages' to see detailed highlights.

# View highlighted passages
> passages

================================================================================
ğŸ“ HIGHLIGHTED PASSAGES (Top Relevant Excerpts)
================================================================================

[Passage 1] Relevance: 87%
Source: wikipedia Machine Learning
ğŸ“ "Supervised learning is a machine learning paradigm where the algorithm
     learns from labeled training data to make predictions on new data."
--------------------------------------------------------------------------------

[Passage 2] Relevance: 76%
Source: wikipedia Machine Learning
ğŸ“ "In supervised learning, each training example consists of an input object
     and a desired output value, allowing the algorithm to learn the mapping."
--------------------------------------------------------------------------------

[Passage 3] Relevance: 68%
Source: wikipedia Machine Learning
ğŸ“ "Common supervised learning tasks include classification and regression."
--------------------------------------------------------------------------------

[Passage 4] Relevance: 65%
Source: wikipedia Machine Learning
ğŸ“ "Examples of supervised learning algorithms include decision trees, random
     forests, support vector machines, and neural networks."
--------------------------------------------------------------------------------

[Passage 5] Relevance: 62%
Source: wikipedia Machine Learning
ğŸ“ "The quality of a supervised learning model depends heavily on the quality
     and quantity of labeled training data."
================================================================================
```

### Use Cases

**1. Research & Citation**

```
User writing a paper needs exact quotes:
  > query What are the limitations of supervised learning?
  > passages
  â†’ Copy-paste highlighted passages directly into paper with attribution
```

**2. Fact-Checking**

```
User skeptical about answer:
  > query How many goals did Ronaldo score?
  Answer: "Ronaldo scored 450 goals for Real Madrid."
  > passages
  [Passage 1] "...scored 450 goals during his nine seasons with Real Madrid..."
  â†’ User verifies claim is grounded in source
```

**3. Debugging Retrieval**

```
Developer sees low-quality answer:
  > passages
  â†’ Check: Are the highlighted passages actually relevant?
  â†’ If yes: Generation problem (LLM not using context well)
  â†’ If no: Retrieval problem (wrong documents retrieved)
```

**4. Learning & Exploration**

```
Student learning about ML:
  > query What is reinforcement learning?
  > passages
  â†’ Read highlighted sentences for quick overview
  â†’ Each passage links to source for deeper reading
```

### Configuration

```python
# src/config.py

class SearchConfig:
    enable_passage_highlighting: bool = False
    max_passages_per_doc: int = 3  # Passages to extract per document
```

**Tuning max_passages_per_doc:**

```
max_passages_per_doc = 1:
  Result: Only 1 sentence per document (5 docs Ã— 1 = 5 passages)
  Use: Quick overview, minimize noise

max_passages_per_doc = 3 (default):
  Result: Up to 3 sentences per document (5 docs Ã— 3 = 15 passages)
  Use: Balanced coverage

max_passages_per_doc = 5:
  Result: Up to 5 sentences per document (5 docs Ã— 5 = 25 passages)
  Use: Comprehensive view, research applications
```

### Performance

```
Overhead:
  - Sentence tokenization: ~10ms per document
  - Relevance scoring: ~2ms per sentence
  - Total: ~30-50ms for typical query (5 docs, 50 sentences)

Memory:
  - Store passages in RAGResponse
  - Typical size: 5-15 passages Ã— 100 chars = 1-2 KB

Impact: Negligible (< 3% of total query latency)
```

### Advanced: Semantic Passage Scoring (Future Enhancement)

**Current approach:** Keyword overlap (simple, fast, 75% accuracy)

**Alternative:** Embedding-based scoring (accurate, slower)

```python
def semantic_passage_scoring(
    sentence: str,
    query_embedding: np.ndarray,
    answer_embedding: np.ndarray
) -> float:
    """Use embeddings for more accurate relevance"""
    sentence_embedding = embed(sentence)

    query_sim = cosine_similarity(sentence_embedding, query_embedding)
    answer_sim = cosine_similarity(sentence_embedding, answer_embedding)

    score = (query_sim * 0.6) + (answer_sim * 0.4)
    return score
```

**Tradeoff:**
- âœ… More accurate: Captures semantic similarity beyond keywords
- âŒ Slower: Requires embedding 50+ sentences per query
- âŒ More complex: Requires embedding model in passage highlighter

**When to use:**
- High-quality citation finding
- Cross-lingual passage highlighting
- When keyword matching fails (paraphrased content)

### Exercise: Verify Answer Grounding

```bash
# Step 1: Enable all quality features
> rerank
> highlight
> hallucination

# Step 2: Load data and query
> load wikipedia "Machine Learning"
> query What is the difference between supervised and unsupervised learning?

# Step 3: Check grounding
> passages
# Do the highlighted passages support the answer?

> hallucination-report
# Check: Are all claims grounded? (should be HIGH grounding if passages match)

> facts
# Verify: Are individual claims supported by passages?
```

**Expected:** High correlation between passage relevance and hallucination grounding.

### Integration with Other Features

**Passage Highlighting + Reranking:**
```
Reranking â†’ Better documents â†’ Better passages
Higher quality sources â†’ More relevant highlights
```

**Passage Highlighting + Hallucination Detection:**
```
Highlighted passages = Evidence for grounding analysis
Each claim in answer should map to a highlighted passage
```

**Passage Highlighting + Fact Checker:**
```
Fact checker verifies claims against retrieved docs
Passage highlighter shows WHICH sentences support each claim
```

---

# CONCLUSION: Your AI Engineering Journey

## Key Takeaways

1. **RAG solves a real problem:** LLMs without retrieval = hallucinations. RAG + retrieval = grounded answers.

2. **Vector embeddings are powerful:** They capture meaning beyond keywords. Foundation of semantic search.

3. **Hybrid search is practical:** Combine semantic (70%) + keyword (30%) for robust results. Best of both worlds.

4. **Two-stage retrieval wins:** Fast bi-encoder retrieval â†’ accurate cross-encoder reranking = production quality.

5. **Diversity matters:** MMR prevents redundant results. Balance relevance and novelty with Î» parameter.

6. **Evaluation is non-negotiable:** RAGAS metrics (faithfulness, relevance, context) ensure quality. Measure everything.

### Immediate Actions
1. **Enable all features:** Run queries with reranking + highlighting enabled
2. **Compare metrics:** Test with/without reranking, measure RAGAS score improvements
3. **Verify grounding:** Use passage highlighting to validate every answer
4. **Tune parameters:** Experiment with MMR Î», reranking thresholds

### Skill Development
1. **Deep dive research papers:**
   - Cross-encoders: "Sentence-BERT" (Reimers & Gurevych, 2019)
   - MMR: "Use of MMR for Diversity-Based Reranking" (Carbonell & Goldstein, 1998)
   - RAG: "Retrieval-Augmented Generation" (Lewis et al., 2020)

2. **Extend the system:**
   - Add semantic passage scoring (embedding-based)
   - Implement HyDE (Hypothetical Document Embeddings)
   - Build contextual compression for passages
   - Add parent-child chunk retrieval

3. **Production readiness:**
   - Deploy with FastAPI
   - Add monitoring (Prometheus, Grafana)
   - Implement user authentication
   - Create web UI (React/Streamlit)

### Advanced Topics to Explore
1. **Fine-tuning:** Train custom reranker on domain-specific data
2. **Multi-modal RAG:** Handle images, tables, diagrams
3. **Agent frameworks:** Add tool use, function calling, ReAct loops
4. **Evaluation at scale:** Build test suites with 1000+ queries
5. **Cost optimization:** Cache cross-encoder results, batch processing

By completing this guide, you now understand:

### Retrieval Fundamentals
- âœ… **Embeddings**: Dense vector representations of text meaning
- âœ… **Semantic search**: Find documents by meaning (cosine similarity)
- âœ… **Keyword search**: Find documents by terms (BM25)
- âœ… **Hybrid search**: Weighted ensemble of both approaches

### Advanced Retrieval
- âœ… **Cross-encoder reranking**: Joint query-document encoding for accuracy
- âœ… **Bi-encoder vs Cross-encoder**: Speed vs accuracy tradeoffs
- âœ… **MMR (Maximal Marginal Relevance)**: Balancing relevance and diversity
- âœ… **Two-stage retrieval**: Fast recall â†’ accurate precision

### Quality & Evaluation
- âœ… **RAGAS metrics**: Context relevance, answer relevance, faithfulness
- âœ… **Hallucination detection**: Grounding analysis and risk scoring
- âœ… **Fact checking**: Claim-level verification
- âœ… **Confidence scoring**: Quantifying answer reliability

### Reasoning Patterns
- âœ… **Query expansion**: Generate variations for broader coverage
- âœ… **Multi-hop reasoning**: Sequential step-by-step question decomposition
- âœ… **Self-query decomposition**: Parallel multi-aspect query splitting

### Production Features
- âœ… **Domain guard**: Detect out-of-domain queries
- âœ… **LRU caching**: Performance optimization for embeddings
- âœ… **Passage highlighting**: Source attribution and transparency
- âœ… **Streaming responses**: Real-time token-by-token output

### Software Engineering
- âœ… **Orchestrator pattern**: RAGSystem coordinates all components
- âœ… **Abstract base classes**: Storage, Retriever, Chunker interfaces
- âœ… **Strategy pattern**: Hybrid search combines multiple strategies
- âœ… **Dataclasses**: Type-safe models (RetrievedDocument, RAGResponse)

## Architecture You Built

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                          USER QUERY                              â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                             â”‚
                    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”€â”€â”
                    â”‚  RAGSystem       â”‚ (Orchestrator)
                    â”‚  (Core)          â”‚
                    â””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                             â”‚
        â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
        â”‚                    â”‚                    â”‚
   â”Œâ”€â”€â”€â”€â–¼â”€â”€â”€â”€â”         â”Œâ”€â”€â”€â”€â–¼â”€â”€â”€â”€â”         â”Œâ”€â”€â”€â”€â–¼â”€â”€â”€â”€â”
   â”‚Retrievalâ”‚         â”‚Reasoningâ”‚         â”‚Evaluationâ”‚
   â””â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”˜         â””â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”˜         â””â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”˜
        â”‚                   â”‚                    â”‚
        â”‚                   â”‚                    â”‚
   â”Œâ”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”     â”‚    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
   â”‚ HybridSearch     â”‚     â”‚    â”‚ RAGAS Evaluator            â”‚
   â”‚ â€¢ Semantic       â”‚     â”‚    â”‚ Hallucination Detector     â”‚
   â”‚ â€¢ BM25           â”‚     â”‚    â”‚ Fact Checker               â”‚
   â””â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜     â”‚    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
        â”‚                   â”‚
   â”Œâ”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”     â”‚
   â”‚ Reranker         â”‚     â”‚
   â”‚ â€¢ Cross-encoder  â”‚     â”‚
   â”‚ â€¢ MMR diversity  â”‚     â”‚
### Documentation
- ChromaDB docs: https://docs.trychroma.com
- Sentence Transformers: https://www.sbert.net
- NLTK: https://www.nltk.org
- OpenAI API: https://platform.openai.com/docs

### Research Papers
- **RAG**: ["Retrieval-Augmented Generation for Knowledge-Intensive NLP Tasks"](https://arxiv.org/abs/2005.11401) (Lewis et al., 2020)
- **Sentence-BERT**: ["Sentence-BERT: Sentence Embeddings using Siamese BERT-Networks"](https://arxiv.org/abs/1908.10084) (Reimers & Gurevych, 2019)
- **Cross-Encoders**: ["MS MARCO: A Human Generated MAchine Reading COmprehension Dataset"](https://arxiv.org/abs/1611.09268)
- **MMR**: ["The Use of MMR, Diversity-Based Reranking"](https://www.cs.cmu.edu/~jgc/publication/The_Use_MMR_Diversity_Based_LTMIR_1998.pdf) (Carbonell & Goldstein, 1998)
- **BM25**: ["The Probabilistic Relevance Framework: BM25 and Beyond"](http://www.staff.city.ac.uk/~sbrp622/papers/foundations_bm25_review.pdf)

### Concepts Deep Dive
- Vector embeddings: https://platform.openai.com/docs/guides/embeddings
- RAGAS framework: https://docs.ragas.io
- Hybrid search: https://www.pinecone.io/learn/hybrid-search-intro
- LLM hallucinations: ["Survey of Hallucination in NLG"](https://arxiv.org/abs/2202.03629)

### Tools & Frameworks
- LangChain: https://python.langchain.com
- LlamaIndex: https://www.llamaindex.ai
- HuggingFace: https://huggingface.co
- Weights & Biases: https://wandb.ai
   â”‚ â€¢ Score          â”‚     â”‚
   â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜     â”‚
                             â”‚
                    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”€â”€â”
                    â”‚ Query Expander   â”‚
                    â”‚ MultiHop Reasonerâ”‚
                    â”‚ SelfQuery Decomp â”‚
                    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

## Next Steps

1. **Monitor your metrics:** Run RAGAS evaluation on every query
2. **Iterate on weights:** Experiment with hybrid search weights
3. **Expand test suite:** Add more adversarial cases
4. **Measure business value:** How does RAG help your users?
5. **Learn more:** Research papers on RAG, embeddings, LLMs

## Resources

- ChromaDB docs: https://docs.trychroma.com
- NLTK: https://www.nltk.org
- BM25: https://en.wikipedia.org/wiki/Okapi_BM25
- Embeddings: https://platform.openai.com/docs/guides/embeddings
- LLaMa: https://llama.meta.com

---

**ğŸ“ You're now ready to build production RAG systems!**

Start with small experiments, measure carefully, iterate based on metrics.

The journey from software engineer to AI engineer isn't about replacing learning
â€”it's about adding new tools to your engineering toolkit.

Good luck! ğŸš€

